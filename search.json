[
  {
    "objectID": "189HW4Code.html",
    "href": "189HW4Code.html",
    "title": "Ethan Elasky",
    "section": "",
    "text": "#CS189HW4 Code\nimport numpy as np\nimport sklearn as sk\nimport matplotlib.pyplot as plt\n\n\n#Problem 1\nfrom sklearn import datasets\ndataset = datasets.fetch_lfw_people()\nX = dataset['data']\n\n\nfig = plt.figure()\ncount = 0\nfor i in range(5):\n    for j in range(4):\n        ax = fig.add_subplot(5,4,count+1)\n        ax.imshow(np.reshape(X[count],(62,47)), cmap = 'gray')\n        count = count + 1\n\n\n\n\n\n#Part b\n\nX_av = np.mean(X, axis = 0)\nX_cent = X - X_av\nplt.imshow(np.reshape(X_av, (62,47)), cmap = plt.cm.gray)\n\n<matplotlib.image.AxesImage at 0x1f27d6fde40>\n\n\n\n\n\n\n#Part c\nX_av.shape\n\n(2914,)\n\n\n\ndef my_pca(X,k):\n    U,sig,V = np.linalg.svd(np.transpose(X) @ X)\n    U_pca = U[:,0:k]\n    sig_pca = np.diag(sig[0:k])\n    Vt_pca = V[0:k, :]\n    return X@np.transpose(Vt_pca)@Vt_pca\n\n\nX_rec10 = my_pca(X_cent, 10)\nX_rec100 = my_pca(X_cent, 100)\nX_rec1000 = my_pca(X_cent, 1000)\n\n\nX_rec10.shape\n\n(13233, 2914)\n\n\n\n#Projections from 10 PCDs\nfig = plt.figure()\ncount = 0\nfor i in range(5):\n    for j in range(4):\n        ax = fig.add_subplot(5,4,count+1)\n        ax.imshow(np.reshape(X_rec10[count] + X_av,(62,47)), cmap = 'gray')\n        count = count + 1\n\n\n\n\n\n#Projections from 100 PCDs\nfig = plt.figure()\ncount = 0\nfor i in range(5):\n    for j in range(4):\n        ax = fig.add_subplot(5,4,count+1)\n        ax.imshow(np.reshape(X_rec100[count] + X_av,(62,47)), cmap = 'gray')\n        count = count + 1\n\n\n\n\n\n#Projections from 1000 PCDs\nfig = plt.figure()\ncount = 0\nfor i in range(5):\n    for j in range(4):\n        ax = fig.add_subplot(5,4,count+1)\n        ax.imshow(np.reshape(X_rec1000[count] + X_av,(62,47)), cmap = 'gray')\n        count = count + 1\n\n\n\n\n\n#Part d\nk = 20\nU,sig,V = np.linalg.svd(np.transpose(X_cent) @ X_cent)\nU_pca = U[:,0:k]\nsig_pca = np.diag(sig[0:k])\nVt_pca = V[0:k, :]\nfig = plt.figure()\ncount = 0\nfor i in range(5):\n    for j in range(4):\n        ax = fig.add_subplot(5,4,count+1)\n        ax.imshow(np.reshape(Vt_pca[count],(62,47)), cmap = 'gray')\n        count = count + 1\n\n\n\n\n\n#Part e\nvar_exps = []\nu,sig,vt = np.linalg.svd(np.transpose(X_cent) @ X_cent)\nfound_cut = False\nfor k in range(2914):\n    curr_var_exps = sum(sig[0:k])/sum(sig)\n    if found_cut == False and curr_var_exps > 0.95:\n        best_cutoff = k\n        found_cut = True\n    var_exps.append(sum(sig[0:k])/sum(sig))\n\nplt.plot(np.arange(2914), var_exps)\n\n\n\n\n\n#The value printed from this cell is the cutoff to achieve\n#95% of the variance explained\nbest_cutoff\n\n292\n\n\n\n#Part f\nlen(X)*0.8\n\n10586.400000000001\n\n\n\nX_train = X_cent[0:10586, :]\nX_test = X_cent[10587:len(X), :]\n\n\ndef rec_loss(k):\n    U,sig,V = np.linalg.svd(np.transpose(X_train) @ X_train)\n    U_pca = U[:,0:k]\n    sig_pca = np.diag(sig[0:k])\n    Vt_pca = V[0:k, :]\n    norm_train = np.linalg.norm(X_train@np.transpose(Vt_pca)@Vt_pca - X_train, ord = 'fro')\n    norm_test = np.linalg.norm(X_test@np.transpose(Vt_pca)@Vt_pca - X_test, ord = 'fro')\n    train_loss = (1/(X_train.shape[0]*X_train.shape[1]))*(norm_train**2)\n    test_loss = (1/(X_test.shape[0]*X_test.shape[1]))*(norm_test**2)\n    return train_loss, test_loss\n\n\npcds = [10,20,50,100,500,1000,2914]\ntrain_losses = []\ntest_losses = []\nfor k in pcds:\n    trl, tel = rec_loss(k)\n    train_losses.append(trl)\n    test_losses.append(tel)\n\n\nplt.plot(pcds, train_losses, label = \"training loss\")\nplt.plot(pcds, test_losses, label = \"test loss\")\nplt.legend()\nplt.xlabel(\"Number of pcds used in projection\")\nplt.ylabel(\"value of Reconstruction Loss\")\nplt.title(\"Reconstruction Loss for Training and Test Data\")\n\nText(0.5, 1.0, 'Reconstruction Loss for Training and Test Data')\n\n\n\n\n\n\naverage_rl_train = np.mean(train_losses)\naverage_rl_test = np.mean(test_losses)\n\n\n#Here is the average RecLoss for training and test respectively\naverage_rl_train, average_rl_test\n\n(0.009160736200737768, 0.009315588500130373)\n\n\n\n#Question 4\nfrom sklearn import manifold\n\n\ndiabetes = datasets.load_diabetes()\nX = diabetes.data\ny = diabetes.target\ndescr = diabetes.DESCR\n\n\nX_tsne = manifold.TSNE().fit_transform(X)\n\nC:\\Users\\Buckleberry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n  warnings.warn(\nC:\\Users\\Buckleberry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n  warnings.warn(\n\n\n\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c = y, cmap = 'inferno')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x1f27b2c94b0>\n\n\n\n\n\n\n#Part b\n#Our clusters correspond to the x value of our embedded values\n\nX_tsne_info = manifold.TSNE().fit(X)\n\nC:\\Users\\Buckleberry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n  warnings.warn(\nC:\\Users\\Buckleberry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n  warnings.warn(\n\n\n\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c = X_tsne[:,0], cmap = 'inferno')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x1f209b1f640>\n\n\n\n\n\n\n#Part c\n\n\n#Part d\n\n\nfrom sklearn import decomposition\n\nX_pca = decomposition.PCA(n_components=2).fit_transform(X)\n\n\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c = y, cmap = 'inferno')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x1f209412a70>\n\n\n\n\n\n\n#Part e\nX_train = X[:100]\nX_test = X[-342:]\ny_train = y[:100]\ny_test = y[-342:]\n\n\nb_col = np.ones(100)\nb_col.reshape([1,100])\nX_train_mod = np.insert(X_train,10,1, axis = 1)\n\n\nwb_ols = np.linalg.inv(np.transpose(X_train_mod)@X_train_mod)@np.transpose(X_train_mod)@y_train\n\n\nb_ols = wb_ols[10]\nw_ols = wb_ols[0:10]\n\n\ntest_mse = (1/len(y_test))*np.linalg.norm(X_test@w_ols + b_ols*np.ones(342) - y_test)**2\ntest_mse\n\n3430.9233826005247\n\n\n\ny_test_pred = X_test@w_ols + b_ols*np.ones(342)\nnum_cor = 0\nnum_discor = 0\nfor i in range(len(y_test)):\n    for j in range(len(y_test)):\n        if i != j:\n            if y_test[i] > y_test[j]:\n                if y_test_pred[i] > y_test_pred[j]:\n                    num_cor += 1\n                else:\n                    num_discor += 1\nC_index = num_cor/(num_cor + num_discor)\nC_index\n\n0.7452930850514576\n\n\n\n#Part f\nX_train = X[:100]\nX_test = X[-342:]\ny_train = y[:100]\ny_test = y[-342:]\n\n\ndef ridge(X,y,lam):\n    y_train = y\n    X_train = np.insert(X,10,1, axis = 1)\n    wb_ols = np.linalg.inv(np.transpose(X_train)@X_train + lam*np.identity(X_train.shape[1]))@np.transpose(X_train)@y_train\n    b_ols = wb_ols[-1]\n    w_ols = wb_ols[0:10]\n    return w_ols,b_ols\n\ndef pca_reg(X,y,k):\n    y_train = y\n    X_train = X\n    X_pca = decomposition.PCA(n_components=k).fit(X_train)\n    mean = np.repeat([X_pca.mean_], repeats=X_train.shape[0], axis=0)\n    U,sig,V = np.linalg.svd(np.transpose(X_train) @ X_train)\n    U_pca = U[:,0:k]\n    sig_pca = np.diag(sig[0:k])\n    Vt_pca = V[0:k, :]\n    X_train = (X_train - mean)@np.transpose(Vt_pca)\n    X_train = np.insert(X_train,1,1, axis = 1)\n    wb_ols = np.linalg.inv(np.transpose(X_train)@X_train)@np.transpose(X_train)@y_train\n    if k == 1:\n        b_ols = wb_ols[0]\n        w_ols = 1\n    b_ols = wb_ols[-1]\n    w_ols = wb_ols[0:k]\n    return w_ols,b_ols,mean,np.transpose(Vt_pca)\n    \n    \ndef cross_val(X,y,type, num_cv, lambs = None, ks = None):\n    val_mses = np.ones(num_cv)\n    train_mses = np.ones(num_cv)\n    fold_size = X.shape[0]//num_cv\n    \n    for i in range(num_cv):\n        if i == 0:\n            X_val = X[0:fold_size]\n            y_val = y[0:fold_size]\n            X_train = X[(fold_size - X.shape[0]):]\n            y_train = y[(fold_size - X.shape[0]):]\n        elif i == 9:\n            X_val = X[-fold_size:]\n            y_val = y[-fold_size:]\n            X_train = X[:90]\n            y_train = y[:90]\n        else:\n            X_val = X[i*fold_size:((i+1)*fold_size - 1)]\n            y_val = y[i*fold_size:((i+1)*fold_size - 1)]\n            X_train = np.r_[X[:i*fold_size],X[(i)*fold_size:]]\n            y_train = np.r_[y[:i*fold_size],y[(i)*fold_size:]]\n        if type == 'r':\n            w_ols,b_ols = ridge(X_train, y_train, lambs[i])\n            train_mse = (1/len(y_train))*np.linalg.norm(X_train@w_ols + b_ols*np.ones(len(y_train)) - y_train)**2\n            val_mse = (1/len(y_val))*np.linalg.norm(X_val@w_ols + b_ols*np.ones(len(y_val)) - y_val)**2\n            val_mses[i] = val_mse\n            train_mses[i] = train_mse\n        if type == \"p\":\n            w_ols,b_ols,mean,pcds = pca_reg(X_train,y_train,ks[i])\n            train_mse = (1/len(y_train))*np.linalg.norm((X_train-mean)@pcds@w_ols + b_ols*np.ones(len(y_train)) - y_train)**2\n            val_mse = (1/len(y_val))*np.linalg.norm((X_val-mean[0:X_val.shape[0]])@pcds@w_ols + b_ols*np.ones(len(y_val)) - y_val)**2\n            val_mses[i] = val_mse\n            train_mses[i] = train_mse\n        if type == 'k':\n            knn = KNeighborsRegressor(n_neighbors = ks[i])\n            knn.fit(X_train, y_train)\n            y_train_pred = knn.predict(X_train)\n            y_predict = knn.predict(X_val)\n            train_mse = np.mean((y_train_pred - y_train)**2)\n            val_mse = np.mean((y_predict - y_val)**2)\n            val_mses[i] = val_mse\n            train_mses[i] = train_mse\n\n    return train_mses, val_mses\n\n\nlambdas = [10**(-9),10**(-8),10**(-7),10**(-6),10**(-5),10**(-4),10**(-3),10**(-2),10**(-1),1]\ntrain_mses,val_mses = cross_val(X_train, y_train, 'r', lambs = lambdas, num_cv = 10 )\n\n\nplt.plot(np.log10(lambdas)[-6:], train_mses[-6:], label = \"training mses\")\nplt.plot(np.log10(lambdas)[-6:], val_mses[-6:], label = \"validation mses\")\nplt.legend()\nplt.xlabel(\"Log base 10 lambda\")\nplt.ylabel(\"MSE\")\nplt.title(\"Vlidation and Training Error for Ridge CV\")\n\nText(0.5, 1.0, 'Vlidation and Training Error for Ridge CV')\n\n\n\n\n\n\n#Based on my plot, I would say our best value for lambda is 10^-3 as it \n\n\n#Here we calculate the test_mse with our new lambda optimization\nbest_lambda = 10**(-3)\nw_best,b_best = ridge(X_train, y_train, best_lambda)\ntest_mse = (1/len(y_test))*np.linalg.norm(X_test@w_best + b_best*np.ones(len(y_test)) - y_test)**2\ntest_mse\n\n3398.1572795040015\n\n\n\n#Now we calculate the C-index with our best lambda\ny_test_pred = X_test@w_best + b_best*np.ones(342)\nnum_cor = 0\nnum_discor = 0\nfor i in range(len(y_test)):\n    for j in range(len(y_test)):\n        if i != j:\n            if y_test[i] > y_test[j]:\n                if y_test_pred[i] > y_test_pred[j]:\n                    num_cor += 1\n                else:\n                    num_discor += 1\nC_index = num_cor/(num_cor + num_discor)\nC_index\n\n0.745688913365229\n\n\n\n#part g\nks = np.arange(10) + 1\ntrain_mses,val_mses = cross_val(X_train, y_train, 'p', ks = ks, num_cv = 10 )\n\n\nplt.plot(ks, train_mses, label = \"training mses\")\nplt.plot(ks, val_mses, label = \"validation mses\")\nplt.legend()\nplt.xlabel(\"K value (# of principal components)\")\nplt.ylabel(\"MSE\")\nplt.title(\"Vlidation and Training Error for PCA CV\")\n\nText(0.5, 1.0, 'Vlidation and Training Error for PCA CV')\n\n\n\n\n\n\n#From this plot I would say that k = 8 does the best at minimizing both MSE values\nbest_k = 8\n\n\n#Here I print our best k test mse\nw_best,b_best,mean,pcds = pca_reg(X_train, y_train, best_k)\nmean = np.repeat([mean[0]], repeats=342, axis=0)\ntest_mse = (1/len(y_test))*np.linalg.norm((X_test-mean)@pcds@w_best + b_best*np.ones(len(y_test)) - y_test)**2\ntest_mse\n\n11263.660145902464\n\n\n\n#Now we calculate the C-index with our best k\ny_test_pred = (X_test-mean)@pcds@w_best + b_best*np.ones(342)\nnum_cor = 0\nnum_discor = 0\nfor i in range(len(y_test)):\n    for j in range(len(y_test)):\n        if i != j:\n            if y_test[i] > y_test[j]:\n                if y_test_pred[i] > y_test_pred[j]:\n                    num_cor += 1\n                else:\n                    num_discor += 1\nC_index = num_cor/(num_cor + num_discor)\nC_index\n\n0.5214435686503975\n\n\n\n#It appears that our C-index has become worse\n\n\n#Part h\nX_train = X[:342]\nX_test = X[-100:]\ny_train = y[:342]\ny_test = y[-100:]\n\n\nb_col = np.ones(100)\nb_col.reshape([1,100])\nX_train_mod = np.insert(X_train,10,1, axis = 1)\n\nwb_ols = np.linalg.inv(np.transpose(X_train_mod)@X_train_mod)@np.transpose(X_train_mod)@y_train\n\nb_ols = wb_ols[10]\nw_ols = wb_ols[0:10]\n\ntest_mse = (1/len(y_test))*np.linalg.norm(X_test@w_ols + b_ols*np.ones(100) - y_test)**2\nprint(\"Test MSE for OLS\")\ntest_mse\n\nTest MSE for OLS\n\n\n2693.859913333543\n\n\n\ny_test_pred = X_test@w_ols + b_ols*np.ones(100)\nnum_cor = 0\nnum_discor = 0\nfor i in range(len(y_test)):\n    for j in range(len(y_test)):\n        if i != j:\n            if y_test[i] > y_test[j]:\n                if y_test_pred[i] > y_test_pred[j]:\n                    num_cor += 1\n                else:\n                    num_discor += 1\nC_index = num_cor/(num_cor + num_discor)\nprint(\"C-index for OLS\")\nC_index\n\nC-index for OLS\n\n\n0.7677955789900629\n\n\n\nlambdas = [10**(-9),10**(-8),10**(-7),10**(-6),10**(-5),10**(-4),10**(-3),10**(-2),10**(-1),1]\ntrain_mses,val_mses = cross_val(X_train, y_train, 'r', lambs = lambdas, num_cv = 10 )\n\nplt.plot(np.log10(lambdas)[-6:], train_mses[-6:], label = \"training mses\")\nplt.plot(np.log10(lambdas)[-6:], val_mses[-6:], label = \"validation mses\")\nplt.legend()\nplt.xlabel(\"Log base 10 lambda\")\nplt.ylabel(\"MSE\")\nplt.title(\"Vlidation and Training Error for Ridge CV\")\n\nText(0.5, 1.0, 'Vlidation and Training Error for Ridge CV')\n\n\n\n\n\n\nbest_lambda = 10**(-2)\nw_best,b_best = ridge(X_train, y_train, best_lambda)\ntest_mse = (1/len(y_test))*np.linalg.norm(X_test@w_best + b_best*np.ones(len(y_test)) - y_test)**2\ntest_mse\n\n2728.718136235313\n\n\n\n#part g\nks = np.arange(10) + 1\ntrain_mses,val_mses = cross_val(X_train, y_train, 'p', ks = ks, num_cv = 10 )\n\n\nplt.plot(ks, train_mses, label = \"training mses\")\nplt.plot(ks, val_mses, label = \"validation mses\")\nplt.legend()\nplt.xlabel(\"K value (# of principal components)\")\nplt.ylabel(\"MSE\")\nplt.title(\"Vlidation and Training Error for PCA CV\")\n\nText(0.5, 1.0, 'Vlidation and Training Error for PCA CV')\n\n\n\n\n\n\n#Here I print our best k test mse\nbest_k = 3\nw_best,b_best,mean,pcds = pca_reg(X_train, y_train, best_k)\nmean = np.repeat([mean[0]], repeats=100, axis=0)\ntest_mse = (1/len(y_test))*np.linalg.norm((X_test-mean)@pcds@w_best + b_best*np.ones(len(y_test)) - y_test)**2\ntest_mse\n\n6184.424090137966\n\n\n\n#In general it appears that as we increase ntrain from 100 to 300\n#There is an increase in accuracy of performance for each method. \n#Particulary, it appears that each method is getting closer to the \n#OLS value of Test MSE, which is what I would expect. More training \n#data allows for our classifier to be more specialized and have a \n#better understanding of the underlying population. \n\n\n#Part i\nfrom sklearn.neighbors import KNeighborsRegressor\nX_train = X[:342]\nX_test = X[-100:]\ny_train = y[:342]\ny_test = y[-100:]\n\n\nks = [1,2,3,5,10,20,30,50,100,4]\ntrain_mses,val_mses = cross_val(X_train, y_train, 'k', ks = ks, num_cv = 10 )\n\n\nplt.plot(ks[0:9], train_mses[0:9], label = \"training mses\")\nplt.plot(ks[0:9], val_mses[0:9], label = \"validation mses\")\nplt.legend()\nplt.xlabel(\"K value (# of principal components)\")\nplt.ylabel(\"MSE\")\nplt.title(\"Vlidation and Training Error for PCA CV\")\n\nText(0.5, 1.0, 'Vlidation and Training Error for PCA CV')\n\n\n\n\n\n\nval_mses\n#It is clear that our lowest validation mse occurs \n#at k = 3 neighbors\n\nbest_k = 3\n\n\nknn = KNeighborsRegressor(n_neighbors = 3)\nknn.fit(X_train, y_train)\ny_predict = knn.predict(X_test)\ntest_mse = np.mean((y_predict - y_test)**2)\nprint(\"Our best test mse for the best k in our cross validation is:\")\ntest_mse\n\nOur test mse for the best k in our cross validation is:\n\n\n3896.5122222222217\n\n\n\nnum_cor = 0\nnum_discor = 0\nfor i in range(len(y_test)):\n    for j in range(len(y_test)):\n        if i != j:\n            if y_test[i] > y_test[j]:\n                if y_predict[i] > y_predict[j]:\n                    num_cor += 1\n                else:\n                    num_discor += 1\nC_index = num_cor/(num_cor + num_discor)\nprint(\"The C-Index for our best k (3) is:\")\nC_index\n\n0.7144595416751166"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "hello.html",
    "href": "hello.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ethan Elasky",
    "section": "",
    "text": "Student at the University of California, Berkeley. Data Science major. Member of Angus Hildreth’s Social Psychology and Business Lab at the Haas School of Business."
  }
]