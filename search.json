[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Ethan Elasky’s Blog",
    "section": "",
    "text": "A Scalable Workflow for Herding AI Agents Toward Your Goals\n\n\nPractices for getting compounding leverage out of Claude Code’s subagent system — persistent specs, scope declarations, task parallelization, and more.\n\n\n\n\n\nFeb 19, 2026\n\n\nEthan Elasky\n\n\n\n\n\n\n\n\n\n\n\n\n“Debate” is not an activity\n\n\nOr at least, not a monolithic one\n\n\n\n\n\nJun 21, 2025\n\n\nEthan Elasky\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on Taiwanese Indie Music\n\n\nFresh songs from across the planet\n\n\n\n\n\nDec 29, 2023\n\n\nEthan Elasky\n\n\n\n\n\n\n\n\n\n\n\n\nAmazon Photos collects your photo data – you can disable this\n\n\nDon’t become Amazon’s training data\n\n\n\n\n\nNov 15, 2023\n\n\nEthan Elasky\n\n\n\n\n\n\n\n\n\n\n\n\nEverything to Know About Flight Delays (no code)\n\n\nAnalyzing post-Covid airline records\n\n\n\n\n\nJul 30, 2023\n\n\nEthan Elasky\n\n\n\n\n\n\n\n\n\n\n\n\nEverything to Know About Flight Delays\n\n\nAnalyzing post-Covid airline records with Python\n\n\n\n\n\nJul 30, 2023\n\n\nEthan Elasky\n\n\n\n\n\n\n\n\n\n\n\n\nImproving Argument Extensions in Lincoln-Douglas debate\n\n\nHow to explain your arguments clearly\n\n\n\n\n\nJul 24, 2020\n\n\nEthan Elasky\n\n\n\n\n\n\n\n\n\n\n\n\nAvoiding overfitting in Lincoln-Douglas debate practice\n\n\nKeeping your skills robust against new arguments\n\n\n\n\n\nMay 28, 2020\n\n\nEthan Elasky\n\n\n\n\n\n\n\n\n\n\n\n\nEmbedded Clash - a Primer for Lincoln-Douglas debate\n\n\nGetting through your flow faster than ever\n\n\n\n\n\nMay 28, 2020\n\n\nEthan Elasky\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/flights-nontechnical.html",
    "href": "posts/flights-nontechnical.html",
    "title": "Everything to Know About Flight Delays (no code)",
    "section": "",
    "text": "As a teenager, when booking flights, my mom always insisted that we book the earliest flight possible. It made no sense to me. “But we’ll have wasted a day traveling anyway, Mom! Why should we wake up super early and be tired just to get to the hotel and do nothing? Why not just book a flight for later in the day?” Years later, I saw her have the same argument with my sister. Having studied data science at UC Berkeley, I realized that this question was answerable with more than just anecdotes about delays.\nThat’s why I decided to study flight delays in-depth and write an article summarizing my conclusions. This article analyzes post-Covid flight information up to February 2023 (the latest available as of today) while sparing the reader the details of data processing."
  },
  {
    "objectID": "posts/flights-nontechnical.html#downloading-the-data",
    "href": "posts/flights-nontechnical.html#downloading-the-data",
    "title": "Everything to Know About Flight Delays (no code)",
    "section": "Downloading the data",
    "text": "Downloading the data\nThe best resource to investigate the regularity of flights within the US is the Bureau of Transportation Statistics. They have a website that hosts the data, which is publicly available. The data is available by month, meaning that I had to manually request data for the 14 months I was interested in.\nI automated data downloading using Python (a common programming language) to request the data in batches directly from the government server. Then, I compiled the data into a spreadsheet with the data processing tool pandas and did some additional processing to make later operations easier."
  },
  {
    "objectID": "posts/flights-nontechnical.html#plot-of-flight-delays-vs-departure-time",
    "href": "posts/flights-nontechnical.html#plot-of-flight-delays-vs-departure-time",
    "title": "Everything to Know About Flight Delays (no code)",
    "section": "Plot of flight delays vs departure time",
    "text": "Plot of flight delays vs departure time\nLet’s take a precursory view of flight delays in the aggregate and see how they correlate with departure time.\n\n\n\nFigure 1\n\n\nWe see that delays overall are densest between 2pm and 6pm, but the most common delay is less than 10 minutes and occurs between 10am and 12pm. Given that the most common delay is minimal, I wanted to take a better look. We’ll look at significant delays, which I will define to be greater than or equal to 30 minutes in length."
  },
  {
    "objectID": "posts/flights-nontechnical.html#plot-of-significant-and-nonsignificant-delays-vs-departure",
    "href": "posts/flights-nontechnical.html#plot-of-significant-and-nonsignificant-delays-vs-departure",
    "title": "Everything to Know About Flight Delays (no code)",
    "section": "Plot of significant and nonsignificant delays vs departure",
    "text": "Plot of significant and nonsignificant delays vs departure\n\n\n\nFigure 2\n\n\nFrom the chart, we see that both slight and significant delays increase as the day goes on. Significant delays start low at 6am and steadily increase, peaking at 6pm, while slight delays seem to plateau around 10am."
  },
  {
    "objectID": "posts/flights-nontechnical.html#delays-by-airline",
    "href": "posts/flights-nontechnical.html#delays-by-airline",
    "title": "Everything to Know About Flight Delays (no code)",
    "section": "Delays by airline",
    "text": "Delays by airline\nNow, let’s look at delays by airline. I merged wholly-owned subsidiaries with their parent company (a complete list of wholly-owned subsidiary airlines in North America can be found on Wikipedia), as people do not often see their brand nor purchase from them. Some airlines, such as Republic Airline, are regional airlines that fly under multiple airline names; our data does not represent whose banner they fly under for a given flight, making it impossible for us to merge their flights with their contracted carrier.\n\n\n\nFigure 3\n\n\nRegardless of airline, the chance of significant delay is lowest in the early hours of the morning, regardless of airline. From there, the probability of delay steadily increases and peaks in the evening. Then there is a dip around midnight, with delays skyrocketing in the wee hours of the morning. Many of these flights are red-eyes from Alaska and Puerto Rico that occur at the end of an airline’s work day. Flying at the end of the work day increases the chance of delay since planes fly multiple flights in a day, and any delay in an earlier flight can mess up the traffic control schedule for all later flights. Additionally, if delayed for too long, crew duty hours can also exceed the limit. This happened in Southwest’s meltdown last year, where Southwest had insufficient replacement crews and misallocation of planes, leading to mass flight cancellations.\nThe budget airlines (Frontier, Spirit, Southwest, and JetBlue) all have higher rates of delay throughout the day according to this dataset, with the exception of Southwest, which has a delay rate comparable with the non-budget airlines in the morning. However, by 12pm, Southwest’s significant delay rate grows past that of the non-budget airlines and joins its budget peers by 4 or 5 pm.\nBy the numbers:\n\n\n\n\nAirline\n\n\n% Flights Significantly Delayed\n\n\n\n\n\n\nJetBlue Airways\n\n\n22.078514\n\n\n\n\nFrontier Airlines Inc.\n\n\n21.896298\n\n\n\n\nAllegiant Air\n\n\n18.991785\n\n\n\n\nSpirit Air Lines\n\n\n16.958591\n\n\n\n\nSouthwest Airlines Co.\n\n\n15.292280\n\n\n\n\nAmerican Airlines Inc.\n\n\n14.529106\n\n\n\n\nMesa Airlines Inc.\n\n\n13.838768\n\n\n\n\nUnited Air Lines Inc.\n\n\n12.994220\n\n\n\n\nHawaiian Airlines Inc.\n\n\n11.101448\n\n\n\n\nPSA Airlines Inc.\n\n\n11.007379\n\n\n\n\nDelta Air Lines Inc.\n\n\n10.671968\n\n\n\n\nSkyWest Airlines Inc.\n\n\n10.629637\n\n\n\n\nAlaska Airlines Inc.\n\n\n10.458504\n\n\n\n\nRepublic Airline\n\n\n10.361243\n\n\n\n\nEndeavor Air Inc.\n\n\n9.843149\n\n\n\n\nEnvoy Air\n\n\n9.762695\n\n\n\n\nHorizon Air\n\n\n8.873647\n\n\n\n\nIn sum, to minimize the chance of delay, it’s best to choose a non-budget airline and depart as early as possible to avoid a significant delay. Your chances of making a connecting flight, getting to your destination at a reasonable hour, and enjoying a warm, healthy dinner all increase if you leave earlier in the day. It seems like my mom was right after all."
  },
  {
    "objectID": "posts/flights-nontechnical.html#applications-lax-and-bur-and-sfo-and-oak",
    "href": "posts/flights-nontechnical.html#applications-lax-and-bur-and-sfo-and-oak",
    "title": "Everything to Know About Flight Delays (no code)",
    "section": "Applications: LAX and BUR, and SFO and OAK",
    "text": "Applications: LAX and BUR, and SFO and OAK\nSince starting college, I’ve traveled quite regularly on planes between my hometown in Southern California and my university in the San Francisco Bay Area. I was drawn to this topic to get a better sense of which airline and airport combination offered me the fewest overall delays. As a college student with a constrained budget, I often take Southwest to and from school due to its cheap fares and customer service..\nSouthern California common wisdom dictates that one should fly out of Burbank (BUR) whenever possible to avoid the hassle of traveling to and out of Los Angeles (LAX.. Coming to the Bay Area for college, I expected Oakland (OAK) and San Francisco (SFO) to share a similar dynamic. However, I was surprised to hear that some of my friends preferred SFO over OAK. In this section, we will determine whether SFO is better than OAK and BUR is better than LAX.\nI follow a similar approach as before but select only flight records that contain our desired airports.\n\n\n\nFigure 4\n\n\nWe see that BUR has fewer delays than LAX, OAK, and SFO. Burbank is better than LAX, but Oakland and SFO are about the same. Maybe my friends’ preference for SFO compared to OAK stems from something else, but clearly delays don’t play a major role in that decision."
  },
  {
    "objectID": "posts/embedded-clash.html",
    "href": "posts/embedded-clash.html",
    "title": "Embedded Clash - a Primer for Lincoln-Douglas debate",
    "section": "",
    "text": "Embedded clash is a specific method of shortening of transitions on the line-by-line. I’ll give an example that illustrates what I mean.\nSay you have the following flow on a sample 1NC Case page with the following carded arguments (I’ll only show the tags for brevity):\n\nNo China war Circumvention – Trump hates the plan and won’t comply Turn – terrorists steal nuclear weapons in the process of elimination and detonate them\n\nThis case hit, although flawed in its brevity, is emblematic of what many debaters read on JF20. Now presume that you are to answer this case hit. For many, I’m sure that it would sound something like\n\n“They say no China war but they’re wrong – China war is real because…” “They say Trump hates the plan but…” “They say terrorists steal nukes but…”\n\nWhile this phrasing can be useful in some situations, especially when your opponent is unclear/when you can tell your judge is having trouble following along, it’s preferable in most cases to use the following phrasing:\n\n“Yes China war – …“\n\n\n“No circumvention – …”\n\n\n“Terrorists don’t steal nukes – …”\n\nI know it looks like a small change, but I can’t understate its importance. This tool not only will improve your speeches on a technical level, but it will also make you sound more argumentatively formidable. Past a certain skill level embedded clash becomes the default, and not employing it will reduce your speaker points and presence.\nFrom the given example, some may assume it’s only relevant to doing very technical case work, but it’s actually very applicable to any line-by-line analysis. Whether giving the 2NR on topicality, the critique, a disadvantage, or a counterplan, or even a lengthy case dump, it would be best in all cases to practice this style and best in most rounds to employ this method of delivery.\nNote: it may be better to use the former phrasing in a few situations: first, when a judge looks confused or is visibly having trouble flowing you; second, against an opponent who doesn’t signpost well/jumps around a lot; third, when you have to jump around a lot or are making more “overview-y” claims rather than mostly doing a lot of line-by-line work."
  },
  {
    "objectID": "posts/avoiding-overfitting.html",
    "href": "posts/avoiding-overfitting.html",
    "title": "Avoiding overfitting in Lincoln-Douglas debate practice",
    "section": "",
    "text": "In this article, I hope to explain overfitting, a concept from the discipline of machine learning, and apply it to debate.\nOverfitting is traditionally defined as producing a model that is too particular and doesn’t accurately work with future problems.\nWe’ve all experienced overfitting before. Probably the most prominent debate example of this phenomenon is redoing speeches. For me, the process of redoing a speech went in the following order: first, I would produce redo material by doing a practice debate, looking through old flows, or watching a debate online. Then, I’d re-give the target speech several times, first with myself and then later with a coach to listen and give feedback. In the end, I’d always feel like I’d improved a lot. For example, I seriously drilled this 2NR from the 2019 National Debate Tournament and got really good at it.\nAfter that drill regiment, I expected I would be much better at giving topicality speeches, but I was disappointed to find that I was only good at producing that one speech, all the while not much better at giving other T 2NRs. In retrospect, the problem is clear: the sample size of topicality rounds at that point was miniscule (\\(n = 1\\)), so my model for giving topicality 2NRs was severely overfitted. Many of the arguments from that speech were contextual to the Executive Authority topic, so I was good at explaining the particularities of why it was hard to be neg in context of that resolution, but not in context of any others. After spelling it out like this, the solution seems to be obvious.\nI did what many of you are thinking: redo speeches in context of the current topic! I drilled several 2NRs on topicality, some of which I’ll link to at the end of this article. This allowed me to better abstract (or generalize) my knowledge about what good topicality debating was and thus allowed me to better apply that generalizable knowledge/intuition to future rounds. To make this advice more concrete, if I were looking to improve at topicality, I’d not only practice/block out the generic “limits outweighs aff counter-standards” but also “limits outweighs predictability,” “it’s hard to be neg for ____ reason,” and even standard-level answers like responses to the PICs objection to T-Bare Plurals, etc. To take it to the next level, I’d also drill different kinds of T arguments across different topics to improve the skill of contextualization (i.e. the AFF’s PICs objection might be more persuasive on a topic in which the States counterplan ran rampant, compared to one in which PICs weren’t good).\nIdeally, regardless of the style of debate being pursued, you would be best to eliminate all “unknown unknowns” (https://en.wikipedia.org/wiki/There_are_known_knowns), or things that catch you by complete surprise. Once you do this, you’ll start finding patterns. For example, the PICs objection applies to most topicality arguments that argue that a type of specification is bad (some that come to mind are T-Plural, T-Eliminate, T-Substantial, T-Nearly All, T-The, T-In, etc.), so with a few modifications, frontlines should be cross-applicable between some T debates.\nA word of caution to taking pattern focus too far: you will inevitably lose specificity and contextualization. This is what judges complain about when they say that topicality debates are becoming stale. It’s one thing to know the generic 7-point response to the aff’s PICs objection, but knowing how to make the all-purpose block more specific (and thus far more persuasive) is an entirely different beast. For instance, quantitative limits claims for Nebel T on the 2019 January-February topic about authoritarian regimes may be less applicable to this year’s January-February topic, which deals with far fewer countries (compare 50 regimes on the JF19 topic with 9 countries on the JF20 topic). This should tell that even the same topicality violation should be debated very differently depending on topic. The difference between Nebel T and an entirely different violation, say T-Plural, is even starker!\nIn sum, you should disrupt your brain to prevent overfitting. If you’re getting better at beating case dumps that you’ve put together on your own, ask a coach or a teammate to put together new ones with random cards from around the wiki. Take the case dumps and do them in sequence, focusing on the quality of the first attempt. Obviously, the unfamiliar ones will be harder than the familiar ones, but ideally, you should be working to close the gap between them. This can help you practice getting used to the unfamiliarity of any good 1NC case strategy, because in a real round, your speech should be great the first time, not the tenth. Sample drill ideas:\n\nDrill the topicality rounds below.\nGo through each Lincoln-Douglas topic that you have debated (or college policy topics, if you know how a topic’s meta breaks down) and write down specificities that complicate generic responses (i.e. the dominance of a core negative generic, lack of disadvantage ground, resolution vagueness, hypothetical quantitative number of affirmatives). Then, try to give speeches while noting these nuances. Rounds that I drilled:\nMontgomery Bell Academy GH vs Monta Vista PS at the 2018 Cal RR. Note: the blocks read here are stale because since 2018 people have been transcribing and reading them.\nNorthwestern JW vs Kentucky BT at the 2019 NDT\nMany of Ishan Bhatt’s (unlisted ☹). A few more topicality rounds that may be helpful:\nHarvard-Westlake SM vs Archbishop Mitty JP at the 2020 Stanford Invitational"
  },
  {
    "objectID": "posts/claude-code-workflow.html",
    "href": "posts/claude-code-workflow.html",
    "title": "A Scalable Workflow for Herding AI Agents Toward Your Goals",
    "section": "",
    "text": "TL;DR: I’ve been using Claude Code’s subagent system to build and maintain complex projects with minimal manual oversight. The core pattern: treat your main Claude Code instance as a manager that delegates to subagents, not a laborer that does everything itself. This post covers the practices that make this work — persistent specs, scope declarations, task parallelization with git worktrees, building skills/tooling to automate yourself out of the loop, and command scoping to keep the whole thing running unattended.\nI’ve been iterating on this workflow across several projects — most extensively a debate/AI safety research codebase and an iOS app — over the past few months. What started as “just use subagents” has turned into a fairly specific set of practices that I keep converging on. I originally sent a version of this as a rambling iMessage to a friend and then realized it might be useful to more people, so here it is cleaned up.\nHere is a workflow I’ve been using to get serious, compounding leverage out of Claude Code’s subagent system. The core idea is simple: you have a “review agent” (your main Claude Code instance) whose job is to delegate, supervise, and integrate — not to do the grunt work itself. Think of it as your friend who manages a team. You don’t want your friend’s context window getting clogged with low-level execution, because once it fills up, you’ve effectively lost your best coordinator. Make the subagents do the hard work.\nBelow are the practices that make this actually work at scale."
  },
  {
    "objectID": "posts/claude-code-workflow.html#make-a-spec-and-treat-it-like-your-bible",
    "href": "posts/claude-code-workflow.html#make-a-spec-and-treat-it-like-your-bible",
    "title": "A Scalable Workflow for Herding AI Agents Toward Your Goals",
    "section": "1. Make a Spec and Treat It Like Your Bible",
    "text": "1. Make a Spec and Treat It Like Your Bible\nLLMs don’t have continual learning. They’re also not always great at remembering something you said 10 turns ago. This is why having a written spec that the agentic system can reference is so important — it’s the persistent source of truth that compensates for the model’s lack of memory.\nBuilding a spec from an existing codebase:\n\nHave the review agent dispatch Explore subagents to comprehensively traverse the codebase and produce sub-specs of different areas.\nThe review agent supervises these subagents and integrates their findings into a comprehensive spec.\nOnce the spec exists, dispatch subagents to review functionality against individual parts of the spec.\nThen dispatch subagents to review cross-cutting flows — e.g., research configs, user workflows in an app — that exercise interactions across the codebase. This catches things the earlier, scope-limited review missed.\nSubagents report findings up; the review agent updates the spec. Repeat steps 3–4 until no further issues are found.\n\nYou can also have the review agent command subagents to run actual configs and predict the output before it occurs — how long it will take, what the result will be, etc. The subagent then checks itself against reality, and any divergences get incorporated into the spec. (The spec should maintain a separate section for “bugs” or “behaviors that seem to go against the intended design” — this keeps the intended-behavior spec clean while still tracking known issues in a persistent, referenceable place.)\nBetween steps, the review agent should summarize findings and report its next steps to the user (writing to a log file is ideal). In a perfect world, the human’s only burden is reading a summary at the end and doing a final quality check. The review agent should not present work to you unless it’s either flawless or it has tried at least eight times and is stuck.\nAdding new features:\nHave the review agent dispatch subagents to do relevant exploration, then have it write a draft spec update (make sure your spec is committed so you can revert). Claude Code’s plans are usually pretty good at architecting these draft updates — but you should have the plan agent (or a fresh subagent) iterate on the plan and make it more specific before executing. The devil is in the details: you often don’t discover that something doesn’t work or doesn’t account for edge case X in your codebase until implementation is halfway through, which wrecks your velocity. It’s far cheaper to have an over-detailed plan than to recover from a half-built wrong approach. Have models critique the draft until you’re satisfied. Then follow the same dispatch-review-fix cycle described above."
  },
  {
    "objectID": "posts/claude-code-workflow.html#scope-declaration",
    "href": "posts/claude-code-workflow.html#scope-declaration",
    "title": "A Scalable Workflow for Herding AI Agents Toward Your Goals",
    "section": "2. Scope Declaration",
    "text": "2. Scope Declaration\nIt is helpful for models to know how hard they should try and what quality bar they’re targeting. A code change to a microservice at Facebook is very different from exploratory graph creation for a blog post.\nIdentify the scope you’re working in and push the model to work at the highest echelon of that scope. For example, if you’re writing a machine learning paper, tell the model that outputs should be worthy of an ICML best paper award.\nAvoid overly personal scope definitions like “write this how [specific researcher] would.” Models will shallowly overfit to the name. The exception is if you genuinely want to emulate someone’s style — but if so, make style exploration a separate task that a subagent dedicates its full effort to producing a style spec. Then have your agent reference that spec rather than relying on name-dropping."
  },
  {
    "objectID": "posts/claude-code-workflow.html#task-parallelization",
    "href": "posts/claude-code-workflow.html#task-parallelization",
    "title": "A Scalable Workflow for Herding AI Agents Toward Your Goals",
    "section": "3. Task Parallelization",
    "text": "3. Task Parallelization\nThis is heavily referenced in §1. The primary benefit is speed: many workflows can be parallelized — exploration, review, independent coding tasks.\nSome workflows cannot be parallelized: tasks hitting an external API with a global rate limit, tasks bottlenecked on GPU availability, etc. For these, run sequentially with high internal batch size.\nWhen the review agent faces a complex problem, it should construct a dependency graph and dispatch parallel subagents for independent subtasks (max batch size of ~5 subagents to avoid context compaction problems from overloading the coordinator).\nGit worktrees for parallel agents: If you’re running multiple review agents in parallel (or even if you’re not — you can’t always know), git worktrees are essential. The idea is simple: every agent session gets its own worktree so there are no filesystem conflicts.\n# From main repo, create a worktree with a new branch\ngit worktree add ../myproject-&lt;task-name&gt; -b &lt;branch-name&gt;\ncd ../myproject-&lt;task-name&gt;\n\n# Install dependencies (required for each new worktree)\nuv sync  # or npm install, etc.\nIf your workflow involves a local server, run it on a different port per worktree to avoid conflicts. Before pushing, always sync with main — other agents may have pushed while you worked:\ngit fetch origin main\ngit merge origin/main  # Resolve any conflicts, then push\nClean up after merging:\ncd /path/to/main-repo\ngit worktree remove ../myproject-&lt;task-name&gt;\ngit branch -d &lt;branch-name&gt;\nA good rule of thumb: never edit the main worktree directly. Put this in your CLAUDE.md so agents internalize it. git worktree list is your friend for keeping track of what’s active."
  },
  {
    "objectID": "posts/claude-code-workflow.html#skills-and-know-how",
    "href": "posts/claude-code-workflow.html#skills-and-know-how",
    "title": "A Scalable Workflow for Herding AI Agents Toward Your Goals",
    "section": "4. Skills and Know-How",
    "text": "4. Skills and Know-How\nYou probably have preferences that override the model’s defaults. For example, the pyplot graphs Claude produces out of the box are built for PowerPoint insertion — small fonts, titles present — which is far from paper-quality. (LaTeX figures in ML papers usually omit titles because the caption serves that purpose, and titles waste precious space.)\nYou can encode these preferences in claude.md, but if it starts getting long (&gt;500 lines), break them out into separate skill files that Claude references when needed. You can have Claude create these skills, and choose whether they’re project-specific or general. See the skills documentation for details.\nSkills for automation: Keep a mental tally of where you’re manually involved in the workflow. (If you’re not naturally good at noticing things you want to automate, consider actually writing these down — e.g., in Obsidian. The act of being aware of automation potential is a skill in itself.) For example, if you’re building an iPhone app, you might be scrolling through screens and noting issues by hand. This is fine as a quality check but grows tedious fast — especially when you tell Claude to fix things, it comes back, and half the fixes didn’t work while one or two introduced new bugs.\nInvest in upgrading your tooling. For example, Claude and I developed a workflow using screenshots, native iOS swipes, and Maestro to let it run gestures and ingest the resulting screenshots to verify changes went through. Claude is persistent — it’ll work for ~5 turns to fix issues, which resolves the vast majority of cases.\nBlind spots: Be aware of what your tools can’t catch. Claude is good at detecting text overlap in images but bad at catching small spacing issues a human would notice immediately. If you primarily oversee app changes via screenshots, don’t be surprised when it can’t tell your app is laggy somewhere. Sometimes you can add tools to compensate; sometimes you can’t, and a notification system for when problems arise is valuable here. (As of this writing, a Claude Code WhatsApp integration exists as an MCP server that I haven’t tested yet but plan to soon — something like this would be ideal for getting pinged when the agent hits a wall rather than having to babysit it.)\nGeneralization failures: Claude often won’t take a skill you’ve given it in one context and apply it elsewhere. If you say “the keyboard-hiding function is broken on screens X, Y, and Z,” it will often fix only those three screens rather than inspecting all screens for the same issue. Adding “generalize the instructions I give you” to claude.md helps but doesn’t fully solve this. Expect to prompt for generalization more than you would with a competent engineer."
  },
  {
    "objectID": "posts/claude-code-workflow.html#command-scoping",
    "href": "posts/claude-code-workflow.html#command-scoping",
    "title": "A Scalable Workflow for Herding AI Agents Toward Your Goals",
    "section": "5. Command Scoping",
    "text": "5. Command Scoping\nIt’s critical that subagents use native Claude Code tools rather than arbitrarily powerful command-line tools like sed. Powerful tools are likely to trigger human-review prompts, which is death for an automated system you want to leave running for hours.\nIf a subagent hits permission issues, the review agent should first try spinning up a new agent for the same task with elevated permissions, passing along context from the old agent’s trajectory so it doesn’t have to rerun everything. If that still fails, the review agent should handle the task itself between steps. If that triggers a human input request, the review agent should flag it prominently — in the final paragraphs sent to the user and in the summary log, not buried in mid-trajectory output. The user can then either update Claude Code’s permissions (if reasonable) or work with the model to find a more restricted alternative.\nYou may also want to ban certain tools for other reasons, e.g., reading long files that explode context lengths. In these cases, build custom tools that let the model access the information more efficiently (and optionally build yourself a simple FastAPI frontend to explore the same data).\nFor example, here’s what this looks like in practice for one of my projects — a debate/AI safety research codebase where reading transcript and evaluation files directly would blow up the context window. Near the top of the project’s CLAUDE.md:\n**NEVER use the Explore agent, Task tool with Explore, or direct file reads\nfor transcript/evaluation tasks.**\n\nALWAYS use these CLI tools instead:\n\n# Set this prefix for all commands\nUV=\"INPUT_ROOT=. SRC_ROOT=. uv run python\"\n\n# Transcript counts and summary\n$UV tools/transcript_viewer/list_transcripts.py --run-id \"&lt;run&gt;\" --summary\n\n# Unevaluated transcripts\n$UV tools/transcript_viewer/list_transcripts.py --run-id \"&lt;run&gt;\" --status unevaluated\n\n# Evaluations filtered by review status\n$UV tools/transcript_viewer/list_evaluations.py --run-id \"&lt;run&gt;\" --status needs_cc_subagent_review\n\n# View evaluation content (NEVER read .json files directly)\n$UV tools/transcript_viewer/show_content.py --run-id \"&lt;run&gt;\" --filename \"&lt;file&gt;\" --show all"
  },
  {
    "objectID": "posts/claude-code-workflow.html#areas-id-love-input-on",
    "href": "posts/claude-code-workflow.html#areas-id-love-input-on",
    "title": "A Scalable Workflow for Herding AI Agents Toward Your Goals",
    "section": "Areas I’d Love Input On",
    "text": "Areas I’d Love Input On\nThere are two topics I think are valuable but don’t have enough experience with to write about authoritatively:\n\nManaging agentic teams. Most of this post is about one human coordinating one review agent and its subagents. What changes when you have multiple review agents working on different parts of a project simultaneously? How do you handle conflicts, keep specs consistent, and avoid duplicated work at that scale?\nManaging Claude Code while doing other things. Right now, getting the most out of this workflow still requires more babysitting than I’d like. Notification systems, mobile communication (e.g., a Claude Code WhatsApp integration — there’s an MCP server for this I haven’t tested yet), and other “async oversight” patterns seem important but underexplored. How do people stay in the loop without staying at their desk?\n\nIf you’ve figured out good practices for either of these, I’d be very interested to hear about them.\n\nThe meta-principle across all of this: treat the review agent as a manager, not a laborer. Offload execution to subagents. Persist knowledge in specs and skills. Automate your own role out of the loop as much as possible. The ideal state is one where you check in, read a summary, and everything is already done correctly."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ethan Elasky",
    "section": "",
    "text": "I am an independent machine learning researcher supported by Open Philanthropy, studying debate for AI control and scalable oversight.\nPreviously, I was a Research Assistant at Academia Sinica in the Natural Language Processing Lab of Professor Lun-Wei Ku (古倫維). I investigated how reasoning training impacts evidence-based question answering, and conducted my work mostly in Mandarin, which I am fluent in.\nAlongside this research, I was also founding engineer at Acaceta, which was a knowledge solutions platform for medical sales representatives and doctors.\nI earned my bachelor’s degree from the University of California, Berkeley, where I graduated magna cum laude (3.98/4.00, ΦΒΚ) with a major in Data Science, a concentration in Applied Mathematics and Modeling, and a minor in Chinese. At Berkeley, I wrote an honors thesis, in which I applied natural language processing (NLP) techniques to quantify bias in Taiwanese media. I was advised by Lucy Li.\nAt the Haas School of Business, I was a member of Angus Hildreth’s Social Psychology and Business Lab for four semesters (Spring 2022-Spring 2024), where I conducted experiments relating to human values and ethical behavior when they conflicted with organizational pressures.\nI also speak Japanese at an intermediate level (enough to navigate daily life in Japan, though not professional settings).\nIn high school, I was ranked top-50 worldwide in Lincoln-Douglas debate; in college, I coached for two years at DebateDrills, a championship-winning team. I am the author of Lincoln Douglas Debate from First Principles and a debate blog, which together have helped hundreds of students improve at the activity.\nOutside of research, I enjoy reading, road biking, surfing, and playing pickleball, tennis, and golf."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Ethan Elasky",
    "section": "",
    "text": "I am an independent machine learning researcher supported by Open Philanthropy, studying debate for AI control and scalable oversight.\nPreviously, I was a Research Assistant at Academia Sinica in the Natural Language Processing Lab of Professor Lun-Wei Ku (古倫維). I investigated how reasoning training impacts evidence-based question answering, and conducted my work mostly in Mandarin, which I am fluent in.\nAlongside this research, I was also founding engineer at Acaceta, which was a knowledge solutions platform for medical sales representatives and doctors.\nI earned my bachelor’s degree from the University of California, Berkeley, where I graduated magna cum laude (3.98/4.00, ΦΒΚ) with a major in Data Science, a concentration in Applied Mathematics and Modeling, and a minor in Chinese. At Berkeley, I wrote an honors thesis, in which I applied natural language processing (NLP) techniques to quantify bias in Taiwanese media. I was advised by Lucy Li.\nAt the Haas School of Business, I was a member of Angus Hildreth’s Social Psychology and Business Lab for four semesters (Spring 2022-Spring 2024), where I conducted experiments relating to human values and ethical behavior when they conflicted with organizational pressures.\nI also speak Japanese at an intermediate level (enough to navigate daily life in Japan, though not professional settings).\nIn high school, I was ranked top-50 worldwide in Lincoln-Douglas debate; in college, I coached for two years at DebateDrills, a championship-winning team. I am the author of Lincoln Douglas Debate from First Principles and a debate blog, which together have helped hundreds of students improve at the activity.\nOutside of research, I enjoy reading, road biking, surfing, and playing pickleball, tennis, and golf."
  },
  {
    "objectID": "posts/improving-argument-extensions.html",
    "href": "posts/improving-argument-extensions.html",
    "title": "Improving Argument Extensions in Lincoln-Douglas debate",
    "section": "",
    "text": "It’s important to know how to extend arguments correctly. If you fail to give a clear understanding of your arguments to the judge, it’s unlikely they’ll vote for you.\nThe first thing that you should do is explain the thesis or impact of your argument. In my opinion, if you’re extending an individual argument (i.e. a single card), you should always just explain the thesis of it. If you’re extending a position, the choice to explain your overarching thesis or terminal impact should depend on the off/on-case argument you’re going for.\nI’ll list what I think the best top-level overview explanation is for each:\n\nCritique: theory of power or impact description (dependent on critique).\nCounterplan: clear explanation of what each plank does.\nDisadvantage: terminal impact description and comparison.\nTopicality shell: overarching view of the model that the aff should defend. Something similar to “Our interpretation is that the aff should defend ______________.” Then, get into impact comparison.\nOffensive case turn: if it has an external impact, treat it as a disadvantage. If it only straight turns one of the aff’s internal links, explain the thesis of the straight turn.\nDefensive case turn: thesis.\n\nThis list won’t work perfectly for all positions, but it can serve as a guiding tool for when you’re unsure.\nA sample disadvantage extension for the 2020 Base DA could sound something like “Trump lash-out goes nuclear – strikes on Chinese mainland spark catastrophic escalation because the CCP will escalate to de-escalate, thinking that they’ll scare Trump into giving up.” This is a good length for any speech starter; it makes a coherent argument in ~25 words, and it clearly paints a picture for the judge of what your argument is.\nThe second thing to do is frame your argument/position. When I say framing, I mean explaining to the judge why your position should be preferred over your opponent’s. if you’re extending a full position, this can include weighing (think magnitude, timeframe, probability), turns case, or important issues to deal with in the overview. If you’re extending a single piece of evidence, you still need reasons to prefer your cards to your opponent’s—examples include depth/breadth/type/specificity of warrants, author/publication qualifications, or recency. And it’s also important to create clear judge instruction that describes why your comparisons should matter more than the other side’s (i.e. Bostrom, etc).\nThere is a third type of argument resolution that I call judge instruction. It’s important to resolve issues in the judge’s mind and instruct them on how to evaluate the round because if you don’t, you may not be happy with the judge’s defaults. I can think of a few instances that illustrate the importance of this. First, in a debate where the 2NR is a process counterplan that solves the aff with a very questionable net benefit, how should the judge evaluate the net benefit? Is it best for the judge to vote on “any risk” of the disadvantage, or should they hold the negative to the bar of presenting a “complete” argument in the fullest sense of the word? Second, in a policy aff vs K debate, how should the judge evaluate the perm? Is the perm an advocacy or a test of competition? In what cases does the aff get the perm? To answer the last question, it might be best for the negative to say that the aff only gets the permutation if they prove “no link,” but the aff might be better off if the judge thought that the permutation can “shield” some of the link to the aff. It is these framing questions that are mostly unanswered in debates at the 3-3 level and lead to many split decisions in elimination rounds. If you’re interested in reading more about this, Scott Phillips delves into this in this post on HS Impact.\nThis isn’t to say every argument ever needs multiple framing issues; you sorta have to collapse more so you have time to resolve the most important parts of the debate in your favor.\nNow that I’m done with my rant about the importance of framing and judge instruction, we’ll deal with the last part of any argument extension: the line by line.\nThis is the simplest part of extending an argument; you just need to answer the other side’s arguments. You should extend important parts of your argument that you didn’t explain above, if any. I’ve seen many high-level critique debaters extend their link(s) on the permutation, and most for whom politics is the disadvantage du jour always extend uniqueness on the first non-unique argument made by the 2AC/1AR.\nTo summarize, there are three parts to extending any argument: first, a top-level explanation that contextualizes your argument for the judge; second, framing issue(s) that shows the judge how to evaluate the debate in your favor; third, a line-by-line refutation that proves to the judge that your argument doesn’t have glaring holes in it. This is something worth practicing, because regardless of the content of your position, you need to be able to competently explain your position/evidence and compare it/them to the other side’s to win."
  },
  {
    "objectID": "posts/amazon-images.html",
    "href": "posts/amazon-images.html",
    "title": "Amazon Photos collects your photo data – you can disable this",
    "section": "",
    "text": "Amazon Photo grants unlimited photo storage (and 5GB of video storage) to Prime members, which is why I use it to store my photo backups.\nI was peeking around in the Amazon Photos Terms and Conditions recently when I found this:\n\nWe retain image recognition data relating to the photos and videos you store using the Services until you disable the applicable Tagging Feature or your account’s access to the Services is terminated.\n\nAlthough vague, this language suggests that your photos and videos could become training data for their models. To avoid your photos’ and videos’ inclusion, go to Amazon Photo Settings, click on Image Recognition, and toggle Tag Photos to OFF."
  },
  {
    "objectID": "posts/taiwanese_music_1.html",
    "href": "posts/taiwanese_music_1.html",
    "title": "Notes on Taiwanese Indie Music",
    "section": "",
    "text": "Since leaving Taiwan, I’ve delved into the island’s indie music scene, taking recommendations mainly from PTT (Taiwan’s Reddit equivalent), my friend Yang Lee, and the Spotify algorithm. The selections I’ve listed below are eclectic, ranging from siren-like female vocals over jazz piano to poppy, guitar-heavy satire of Japanese magical girl anime. Taiwan’s alternative scene is not merely derivative of Japan’s, the West’s, or anywhere else’s—the music is creative and unique, reminding me of the enormity and potential of what’s still out there to discover.\nNote: Translations are mine. Search for songs using the Chinese name if available. I made a Spotify playlist with every song in this reflection, plus many more.\n\nHello Nico\n\n\n\nImage source: Taiwan Beats\n\n\nHello Nico is not a loud band, but their songs are no less powerful because of it. The lyrics take the form of recollections, looking doubtfully at past decisions. Their lead singer’s soft voice makes for good “sad” music, but they have lots of optimistic music that is at least as good. Selections:\n\nFlower/Hua 花 is a slow, apologetic contemplation of what could have been, had the speaker not unintentionally hurt Hua (a pet name for the subject). Spotify, YouTube\nUnfamiliar Room, Unfamiliar Afternoon 陌生的房間陌生的下午 describes the pain of naively continuing an unreciprocated love. Spotify, YouTube\nFacing Yourself 面向自己 exemplifies “breathiness” in Taiwanese music (which is popular in the Japanese scene) and intelligently uses myriad color and light-based metaphors to describe the uniqueness of the speaker’s romantic interest. Spotify, YouTube\n\n\n\nIruka Porisu (aka Dolphin Police, イルカポリス, 海豚刑警)\n\n\n\nImage source: Zhihu\n\n\nIruka Porisu’s 2019 album Call Me When Night Go Blue is one of my favorites of 2023. Songs effortlessly mesh the lead singer Wuyue’s (伍悅) siren-like voice, intense guitar solos, and unconventional instruments such as a kid’s piano. Their music, lyrics, social media profiles, and videos embody youth—irreverent, chaotic, and joyful—and remind me to enjoy mine while it lasts. Check out their music videos, which I’ve linked below.\nSelections:\n\nLights of Anping 安平之光 describes blithely riding a Vespa around Anping, Tainan (a city in the south of Taiwan), away from depressing Taipei. Spotify, Youtube\nTell Me I Was in Your Dream 當ㄋ沉睡ㄉ時候告訴婐ㄋ夢到ㄌ婐 is an enthralling song about love and longing in a relationship with a pessimist. Spotify, YouTube, YouTube acoustic\nYoung Folks Die Late (original song title is in English) is full of harmonic tension and discusses the previous night’s sexual partner for misunderstanding the speaker. Spotify, YouTube\nBadminton Youth 羽球少年 describes the speaker’s romantic frustrations with men, women, and the available partners in her environment, especially those available on Tinder. Spotify, YouTube\n\n\n\nElephant Gym (大象體操)\n\n\n\nImage source: Taiwan Beats\n\n\nWithin my first month in Taiwan, shortly after discovering Elephant Gym on PTT, I saw them perform at an annual memorial concert that raises awareness for the 228 Incident, an anti-government uprising in 1948 that was violently suppressed by the Chiang Kai-shek regime. Recently, they’ve skyrocketed in popularity in the West and have even been selected as the cover image for Spotify’s global math rock playlist. The trio expertly combines melodies with polyrhythms on guitar, base, and drums, and sometimes even adds in female vocals. Words rarely appear on tracks, and when they do, they are sparing. Hence, I have omitted descriptions of lyrics for this artist.\nThey’re currently on their World Tour. If anyone is interested in seeing the Los Angeles or San Francisco shows, I have tickets to both.\nSelections:\n\nMidway 中途 Spotify, YouTube\nGo Through the Night 穿過夜晚 Spotify, YouTube\nUnderwater 水底 Spotify, YouTube\n\n\n\nSweet John (甜約翰)\n\n\n\nImage source: Musictalk\n\n\nSweet John makes love songs that are introverted in character and content. The music is largely absent of the hysteria and bold proclamations typical of the genre, an interviewer noted in early 2020. Their lyrics instead provide a more realistic treatment of feelings like the anxiety inherent to an ambiguous relationship and of falling out of love in a relationship.\nSelections:\n\nThe Chance of Rainfall 降雨機率 has gotten universal acclaim from everyone I’ve shown it to thus far. “Why can’t Americans make more songs like this?” was my dad’s reaction. Its jazz-inspired instrumentals are as strong as its duet vocals and makes a great introduction to this genre. Spotify, YouTube\nSafety Limit 安全範圍 weaves dramatic guitar riffs and drum rhythms together with a violin part and the singers’ more delicate voices. Spotify, YouTube\nSpark Fades 容易被厭倦的時刻 ponders the longevity of the speaker’s relationship with his significant other. It begins simply with vocals and piano but gradually evolves into an intense guitar and drum-driven climax. Spotify, YouTube"
  },
  {
    "objectID": "posts/flow-lay-debate.html",
    "href": "posts/flow-lay-debate.html",
    "title": "“Debate” is not an activity",
    "section": "",
    "text": "Debate is one of the most misunderstood extracurriculars. It is not a singular entity; “debate” is an umbrella term that encompasses two highly dissimilar activities. Lay debate is a slow, rhetoric-focused activity that appeals to laypeople, while flow debate is a fast, evidence-intensive activity that requires trained judges.\nLay debate is debate judged by laypeople – think parents, volunteers, and sometimes a few coaches. In this format, debaters use a normal cadence and appeal to common sense and traditional values. When people imagine debate, what they think of is something like lay debate – something between presidential debates and Charlie Kirk on YouTube.\nIn fact, the default mode of debate, particularly at lower-prestige debate tournaments, is lay debate. At most county- or state-level tournaments, this is what you’ll encounter. There are two national tournaments that cater to lay debate: the NSDA and NCFL Nationals, both held in June.\nFlow debate is an activity where details matter much more, and it dominates the National Circuit1. People speak at double or triple normal speed, the opening speeches are extremely evidence-heavy, and experienced judges meticulously track arguments from speech to speech. Almost all major national tournament rounds in the Policy Debate and Lincoln Douglas formats use the flow format; with judges generally required to be debate experts. This is somewhat antithetical to lay debate, which is supposed to appeal to the general public. To get a feel for what flow debate looks like, you can watch this collegiate flow debate round."
  },
  {
    "objectID": "posts/flow-lay-debate.html#types-of-resolutionality",
    "href": "posts/flow-lay-debate.html#types-of-resolutionality",
    "title": "“Debate” is not an activity",
    "section": "Types of resolutionality",
    "text": "Types of resolutionality\nOne immediate difference between flow and lay debate is that flow debate usually adopts a different form of resolutionality.\nTo understand this difference, let’s examine a past resolution from the January-February 2019 high school Lincoln-Douglas topic:\n\nResolved: The United States ought not provide military aid to authoritarian regimes.\n\nNote that the phrases “military aid” and “authoritarian regimes” are not specified. At face value, these terms seem like they should be interpreted broadly, which aligns with how lay debate interprets resolutions. But flow affirmatives usually specify a certain type of military aid, particular authoritarian regime(s), or another element in the resolution to narrow the debate’s scope. In other words, flow debate allows participants to make more specific advocacies (“plans”) within the resolution’s bounds. For instance, on the military aid topic above, a flow affirmative might narrow the debate to eliminating arms sales to Saudi Arabia and the UAE, and the negative must be prepared to disagree with that specific affirmative, as well as all other possible specifications. As you might expect, this massively increases the preparation burden for both sides: the affirmative is incentivized to research all possible plans in search of the best one, while the negative must be ready to negate every conceivable affirmative.\nSince negatives must be prepared to debate all possible affirmatives, students can spend up to thousands of hours over the course of their debate career reading academic journal articles, books, and newspapers for high-quality sources and arguments. In fact, in flow debate, such extreme focus is placed on evidence that the first speeches consist almost solely of reading evidence into the record for use in rebuttal speeches. This is in contrast to lay debate, in which it’s possible to spend roughly 50 hours or less on research for a given topic. Flow debaters often develop extremely strong understandings of their topic areas—both the available arguments and their comparative merit, as competitive dynamics tend to push competitors toward the truest arguments.\nWithin the flow debate community, other norms have proliferated. There is a standard of open evidence, meaning both sides must disclose all previously argued positions and evidence online at this wiki. Also, before any debate round, the affirmative must tell the negative side which advocacy they plan to deploy—a practice established to facilitate in-round clash.\nAnother norm is substantial time commitment. Flow debaters often also have no choice but to make debate their “main activity” when they compete, whether in high school or college. Flow debate requires significant commitment and, on top of a difficult course load, can be difficult to pursue seriously alongside other activities. This contrasts to lay debate, which can be done in conjunction with other activities.\nThese cultural differences between the formats become even more pronounced when we consider how each approaches one of debate’s most fundamental and controversial features—the requirement that students argue both sides of an issue."
  },
  {
    "objectID": "posts/flow-lay-debate.html#switching-sides",
    "href": "posts/flow-lay-debate.html#switching-sides",
    "title": "“Debate” is not an activity",
    "section": "Switching Sides",
    "text": "Switching Sides\nOne commonality between both lay and flow debate is the ability to switch sides. According to debate veteran Laurence Zhou, this practice 2\n\nincreases the chance for good faith debates to emerge and helps strengthen your existing beliefs by helping you find weaknesses in them, both desperately needed in an age where partisanship has made good faith debate difficult if not impossible. This process of switching sides forces students who may never have been exposed to more leftist or radical views to evaluate them on their merits. It forces students to think about how to offer a defense of views they may disagree with, challenging previous strawmen they may have previously believed. Debate, then, provides an obvious mechanism for penetrating filter bubbles by exposing students to a range of views they have previously not considered. (Zhou 2021)\n\nFlow debaters have pushed this principle far beyond what most lay tournaments would find acceptable. For over thirty years, flow debaters have been deploying critical theory, normally seen in graduate humanities classrooms, in their debate rounds. Alongside the usual policy and traditional value-level disagreements with an affirmative, negatives now frequently introduce methodological disagreements (“critiques”) using concepts from Marxism, feminism, colonialism, psychoanalysis, and other frameworks of analysis. These concepts typically create debates about epistemology, metaphysics, ontology, and other abstract philosophical concepts, forcing the affirmative to justify their assumptions and methods alongside their policy arguments. Some affirmatives also employ critical arguments in support of the resolution (or perhaps an unconventional interpretation of it), shifting the focus away from the policy consequences of the resolution and towards theory, which further increases the diversity of arguments.\nRecently, people on the right like James Fishback, founder of neo-traditional debate league Incubate Debate, have been posting out-of-context clips of critical theory affirmatives from flow debate on Twitter. His replies are full of people who assume that yet another institution in American society—this time good old-fashioned high school debate—has been taken over by leftists.\nThere are two problems with this view.\nFirst, debate is by nature internally resolving. It’s not an activity like soccer, where if someone cheats, the game is paused, a referee makes a call, and a punishment is meted out. Debate is self-adjudicating. If an argument is of poor quality or if someone is cheating, then the two teams can use the debate itself to resolve the issue. If an argument seems wrong but the other side can’t come up with a counterargument, the other side should either (a) get better at debate or (b) tailor their strategy accordingly, as this “wrong” argument might just be true.\nIf a team makes an argument like what James describes:\n\n“They have a white debater on their team, which inherently means they have more whiteness than us” argues a **nationally ranked high school debater.**\n\ntheir opponents can and should make arguments that the round should be evaluated on arguments, not identity, and that deciding wins and losses based on identity is a bad model for debate.\nIf an affirmative decides not to present a resolutional advocacy because they would rather talk about, e.g., trans rights, the negative can and should argue that that affirmative should lose. Negatives can talk about why it’s unfair that the affirmative has presented an advocacy that is hard to disagree with, why debate is good, why the resolution should be the center of the debate, and so on.\nSecond is the issue of judges. Some people like James and his reply guys, with another clip as evidence, claim that judges are insanely biased in favor of leftists. In the round I linked, James notes that the judges support the affirmative’s non-resolutional discussion of trans rights. This is evidence that, post-debate, the judges identify with a particular side of an issue, but it is not evidence that the judges violated neutrality during the debate. Judges are instructed to be neutral and encourage debaters to resolve arguments themselves to minimize judge intervention. But even if you think that judges have certain unmitigable biases, there are several other mechanisms in debate that help remedy this issue:\n\nJudge paradigms – judges must post a blurb (you can read mine here) explaining their debate/ideological beliefs. This way, debaters can adapt their arguments to their judge, which is an important part of persuasion and ideological flexibility.\nJudge preferences – pre-tournament, debaters can rank judges based on preference, and during a tournament, after pairings have been made, debaters are assigned a judge (or a panel of judges) with equivalent ratings from each side. Debaters also get a certain number of strikes, which can be assigned instead of a rating to avoid getting that judge. If you think a judge is extremely biased, you can put them at the bottom of your preference card or strike them. Of course, getting judges you’re not entirely philosophically aligned with is unavoidable, but the preference system strikes a good balance between diversity of judging and impartiality to both competitors.\n\nGiven these self-correcting mechanisms within debate, it’s frustrating to me that rightists point to debate as another corrupted institution without understanding the point of switching sides or how flow debate actually works. I agree with the sentiment that flow debate judging should be composed of more diverse professions and demographics, but to have laypeople (or a Twitter mob) judge flow debate would be like having laypeople judge Olympic gymnastics or peer-review physics journal submissions.\nOne way to address the diversity issue directly would be to build a stronger tradition of alumni connections—schools asking former debaters and coaches, many of whom have gone on to careers in every field imaginable—to judge a few times a year for the betterment of high school or college students.\nBeyond bringing in more alumni, we should also encourage students to do flow debate, either as its own activity or alongside lay debate (debaters trained in flow debate often perform extremely well in lay debate). Citing Zhou again, who references extensive academic research, flow debate has the power to motivate students to deeply research, to actively learn (because the stakes of knowledge are victory or loss), to critically think on their feet and logically reason, and to avoid dogmatism and ideological silos.\nAs a debate alumnus, it’s hard for me to overstate the positive impact the activity had on how I think about the world and on my mental capabilities. I hear the same sentiment from fellow debate alumni, most of whom also invested hundreds or thousands of hours into the activity. Yet despite these benefits and the mechanisms that make flow debate work, the general public still doesn’t grasp how switch-side debate solves most of the concerns right-wingers raise, or even recognize that debate is a technically demanding pursuit. If I mentioned to a random person that my main intellectual focus in high school was debate, I’d likely get a puzzled look in return.\nThis disconnect represents a real loss—both for the activity and for society more broadly. The skills developed through flow debate are exactly what we need more of: the ability to research deeply, think critically under pressure, engage with opposing viewpoints in good faith, and avoid ideological dogmatism, understanding that no one side has a monopoly on the truth. Yet debate remains misunderstood, often dismissed as either idealistic policy simulation that ignores politics or, as recent criticisms suggest, political indoctrination.\nI’m not sure what the complete solution to this lack of understanding is, or if there is even a complete one to begin with. Nonetheless, if there is one, it likely requires effort from the debate community itself, as well as those who have left it for other pursuits. Laurence concludes his Foreign Policy article I linked above with one potential solution:\n\nAs former debaters, we should wear our experience with pride, not as a quirky footnote but as a profound influence on our critical faculties and civic engagement. It is time we stepped into the role of educators and advocates, clarifying the unique dynamics of policy-style debate to the public and inviting more diverse participation. In doing so, we build a community that not only appreciates the art of debate but also upholds it as a cornerstone of our democratic society."
  },
  {
    "objectID": "posts/flow-lay-debate.html#footnotes",
    "href": "posts/flow-lay-debate.html#footnotes",
    "title": "“Debate” is not an activity",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFlow debate is near-universal in the Policy and Lincoln-Douglas debate formats on the National Circuit, and it’s extremely prevalent in Public Forum.↩︎\nIf you’re interested in learning more about the benefits of switch-side debate, see here.↩︎"
  },
  {
    "objectID": "posts/Flights-technical.html",
    "href": "posts/Flights-technical.html",
    "title": "Everything to Know About Flight Delays",
    "section": "",
    "text": "This is the technical version of an article that analyzes flight information from January 2022 to February 2023 (the latest available as of today). Of great interest are flight delays.\n\n\nShow the code\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport zipfile\nimport os\nimport requests\nfrom pathlib import Path\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n\n\nThe best resource to investigate the regularity of flights within the US is the Bureau of Transportation Statistics. They have a website that hosts the data, which is publicly available. The data is available by month, meaning that I had to manually request data for the 14 months I was interested in.\nWe can automate this download with the requests library of Python, setting START_YEAR to the beginning year of the period of interest and END_YEAR to the end year (exclusive).\n\n\nShow the code\nSTART_YEAR = 2022\nEND_YEAR = 2024\n\n\n\n\nShow the code\nos.mkdir(\"data\")\n\n\n\n\nShow the code\nfor i in range(START_YEAR, END_YEAR):\n    for j in range(1,13):\n        # We set verify to False because an SSL cert error gets thrown otherwise for some reason.\n        # For some reason, creating a pool manager as described on the Certifi documentation\n        # throws an SSL connection error. It could be an issue with Google Colab, which I am using to\n        # work with this notebook.\n        r = requests.get(f\"https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{i}_{j}.zip\", verify=False)\n        with open(f\"data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{i}_{j}.zip\", \"wb\") as fd:\n            fd.write(r.content)\n\n\nAt this point, we have a bunch of zip files, one for each month of flights. We’ll unzip them one-by-one.\n\n\nShow the code\n#Construct an array to hold our zip filepaths\nzips = []\nfor root, directories, filenames in os.walk('data'):\n    for directory in directories:\n         dpath = os.path.join(root, directory)\n\n    for filename in filenames:\n        fpath = os.path.join(root,filename)\n        if fpath[-3:] == 'zip':\n            zips.append(fpath)\n\nfor zip_path in zips:\n    try:\n        my_zip = zipfile.ZipFile(zip_path, 'r')\n        my_zip.extractall('data')\n    except zipfile.BadZipFile:\n        continue\n\n\n\n\n\nNow, we read the data in and clean it. We start out by reading in an arbitrary month to get a DataFrame with the correct columns, then loop through each month’s flight records (stored in a csv). We add each month’s flight records to our flights DataFrame; at this point, we are done loading data for this project.\nCleaning the DataFrames involves several steps. To start, we read in this HTML table from the Bureau of Transportation Statistics as a DataFrame. This table matches airlines with abbreviations. To aid legibility, we replace the abbreviations contained in our flights DataFrame with the airlines’ full names.\n\n\nShow the code\nimport re\n\n#Obtains DataFrame with correct columns\nflights = pd.DataFrame(columns=pd.read_csv(\"data/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_5.csv\").columns)\n\n#Concatenates inputs to flights DataFrame\nfor root, directories, filenames in os.walk('data'):\n    for filename in filenames:\n        fpath = os.path.join(root,filename)\n        if (fpath[-3:] == 'csv') & (fpath[5:8] == 'On_'):\n            a = pd.read_csv(fpath)\n\n            # Filter out unneeded rows (only keep things relevant to flight origin, airline, and departure delay)\n            a = a.filter(['CRSDepTime', 'DepDelayMinutes', 'Reporting_Airline', 'TaxiOut', 'Origin', 'Flight_Number_Reporting_Airline'], axis=1)\n\n            # Concatenate month table to flights\n            print(f\"  {fpath}\")\n            flights = pd.concat([flights, a], ignore_index=True, join='inner')\n\n# Obtains a DataFrame version of the HTML table on the BTS Airline Codes webpage\ncodes = pd.read_html(\"https://www.bts.gov/topics/airlines-and-airports/airline-codes\")[0]\nflights = flights.merge(right=codes, left_on='Reporting_Airline', right_on='Code', suffixes=['',''])#.drop([\"Reporting_Airline\", 'Code'], axis=1)\n\n\nWe now make a copy of flights called delays. We can now safely transform our data and keep the original flights available for later analysis. We’ll also regularize our time (converting the HHMM format to HH.MM, where MM is out of 100 instead of 60).\n\n\nShow the code\n# Makes a deep copy of flights called delays for transformation in this section. Filter out on-time flights.\nflights['CRSDepTime'] = flights['CRSDepTime'] % 100 * 5/3 * .01 + flights['CRSDepTime'] // 100\nflights['delay'] = flights['DepDelayMinutes'] &gt; 0\ndelays = flights.copy(deep=True)[flights['DepDelayMinutes'] &gt; 0]\n\n\n\n\n\nLet’s take a precursory view of flight delays in the aggregate and see how they correlate with departure time.\n\n\nShow the code\nsns.set_theme()\n\nf, ax = plt.subplots(figsize=(7, 7), sharex=True, sharey=True)\na = sns.histplot(data=delays, x='CRSDepTime', y='DepDelayMinutes', bins=[12,600],\n                 cmap=sns.color_palette(\"flare_r\", as_cmap=True), cbar=True, cbar_kws={'label': 'Quantity of delays'})\nsns.kdeplot(data=delays.sample(1000), x='CRSDepTime', y='DepDelayMinutes', levels=5, color=\"w\")\n\nplt.title('Density of delays by departure time, ' + str(START_YEAR) + \"-\" + str(END_YEAR - 1))\nplt.xlabel('Departure time')\nplt.ylabel('Departure delay, in minutes')\nplt.xlim((0, 24))\nplt.ylim(bottom=0, top=90)\nplt.xticks(range(0,24,4), ['12am','4am','8am','12pm','4pm','8pm'])\n\n\n([&lt;matplotlib.axis.XTick at 0x7fa796f36290&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa796f35c00&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7d57ca740&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7894ad390&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7894ac340&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7894ae500&gt;],\n [Text(0, 0, '12am'),\n  Text(4, 0, '4am'),\n  Text(8, 0, '8am'),\n  Text(12, 0, '12pm'),\n  Text(16, 0, '4pm'),\n  Text(20, 0, '8pm')])\n\n\n\n\n\n\n\n\n\nWe see that delays overall are densest between 2pm and 6pm, but the most common delay is less than 10 minutes and occurs between 10am and 12pm. Given that the most common delay is minimal, I wanted to take a better look. We’ll look at significant delays, which I will define to be greater than or equal to 30 minutes in length.\n\n\n\n\n\nShow the code\ndef delay_significance(x):\n    \"\"\"Categorizes delays into one of three categories.\"\"\"\n    if x == 0:\n        return \"On Time\"\n    elif x &lt; 30 and x &gt; 0:\n        return \"Slight Delay\"\n    else:\n        return \"Significant Delay\"\n\n#Creates labels for hue sorting on histogram.\nflights['Delay Status'] = flights['DepDelayMinutes'].apply(delay_significance)\n\n\n\n\nShow the code\nsns.histplot(flights, x='CRSDepTime', kde=True, hue='Delay Status',\n             binwidth=1, kde_kws={'bw_adjust': 3}).set(xlabel='Departure time', title='Flights from '\n                             + str(START_YEAR) + ' to ' + str(END_YEAR - 1) + ', by hour')\n\nplt.xlim(0,24)\nplt.xticks(range(0,24,4), ['12am','4am','8am','12pm','4pm','8pm'])\n\n\n([&lt;matplotlib.axis.XTick at 0x7fa7849715d0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7849715a0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa784972a40&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7858f0b80&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7858f1630&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7858f20e0&gt;],\n [Text(0, 0, '12am'),\n  Text(4, 0, '4am'),\n  Text(8, 0, '8am'),\n  Text(12, 0, '12pm'),\n  Text(16, 0, '4pm'),\n  Text(20, 0, '8pm')])\n\n\n\n\n\n\n\n\n\nFrom the chart, we see that both slight and significant delays increase as the day goes on. Significant delays start low at 6am and steadily increase, peaking at 6pm, while slight delays seem to plateau around 10am.\n\n\n\nNow, let’s look at delays by airline. We merge wholly-owned subsidiaries with their parent company (a complete list of wholly-owned subsidiary airlines in North America can be found on Wikipedia), as people do not often see their brand nor purchase from them. Some airlines, such as Republic Airline, are regional airlines that fly under multiple airline names; our data does not represent whose banner they fly under for a given flight, making it impossible for us to merge their flights with their contracted carrier.\n\n\nShow the code\n# Adds new departure hour columns to flights and delays for easier charting\nflights['CRSDepHour'], delays['CRSDepHour'] = flights['CRSDepTime'] // 1,  delays['CRSDepTime'] // 1\n\n#Selects only for significant delays\ndelays = delays[delays['DepDelayMinutes'] &gt;= 30]\n\n# Merges wholly-owned subsidiary airlines with their parent companies\ndelays_ma = delays.replace(to_replace={\"Airline\": {\"Horizon Air\":\"Alaska Airlines Inc.\", \"Envoy Air\":\"American Airlines Inc.\", \"PSA Airlines Inc.\":\"American Airlines Inc.\", \"Endeavor Air Inc.\": \"Delta Air Lines Inc.\"}})\nflights_ma = flights.replace(to_replace={\"Airline\": {\"Horizon Air\":\"Alaska Airlines Inc.\", \"Envoy Air\":\"American Airlines Inc.\", \"PSA Airlines Inc.\":\"American Airlines Inc.\", \"Endeavor Air Inc.\": \"Delta Air Lines Inc.\"}})\n\ndef airline_delay_frequencies(airline, delays, flights):\n    \"\"\"Returns delay proportions, grouped by departure hour and departure time.\"\"\"\n    total_delays = delays[delays['Airline'] == airline].groupby('CRSDepHour')['CRSDepTime'].count()\n    total_flights = flights[flights['Airline'] == airline].groupby('CRSDepHour')['CRSDepTime'].count()\n    return (total_delays / total_flights).reindex(np.arange(24), fill_value=0) * 100\n\n# Accumulates multiple airline Series into a DataFrame for line plot\nairlines = ['American Airlines Inc.', #'SkyWest Airlines Inc.',\n       'Alaska Airlines Inc.', 'United Air Lines Inc.',\n       'Delta Air Lines Inc.', 'Frontier Airlines Inc.', #'Allegiant Air',\n       #'Hawaiian Airlines Inc.',\n        'Spirit Air Lines',\n       'Southwest Airlines Co.', #'Mesa Airlines Inc.', 'Republic Airline',\n       'JetBlue Airways']\nproportions = pd.DataFrame()\nfor airline in airlines:\n    proportions[airline] = airline_delay_frequencies(airline, delays_ma, flights_ma)\n\n\n\n\nShow the code\n# Constructs a bar plot of flight delay percentage according to time\na = sns.lineplot(data=proportions, palette=\"tab10\", linewidth=2.5)\nplt.xlabel(\"Scheduled Hour of Flight Departure\")\nplt.ylabel(\"Percentage\")\nplt.title(\"Percent of flights significantly delayed, by time of day\")\nplt.xticks(range(0,24,4), ['12am','4am','8am','12pm','4pm','8pm'])\nplt.ylim(0, 40)\nsns.move_legend(a, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n\n\n\n\n\n\n\nRegardless of airline, the chance of significant delay is lowest in the early hours of the morning, regardless of airline. From there, the probability of delay steadily increases and peaks in the evening. Then there is a dip around midnight, with delays skyrocketing in the wee hours of the morning. Many of these flights are budget airline red-eyes from Alaska and Puerto Rico that occur at the end of an airline’s work day, which can be later than 2am in those areas. Flying at the end of the work day increases the chance of delay since planes fly multiple flights in a day, and any delay in an earlier flight can mess up the traffic control schedule for all later flights. Additionally, if delayed for too long, crew duty hours can also exceed the limit. This happened in Southwest’s meltdow last year, where Southwest had insufficient replacement crews and misallocation of planes, leading to mass flight cancellations.\nThe budget airlines (Frontier, Spirit, Southwest, and JetBlue) all have higher rates of delay throughout the day according to this dataset, with the exception of Southwest, which has a delay rate comparable with the non-budget airlines in the morning. However, by 12pm, Southwest’s significant delay rate grows past that of the non-budget airlines and joins its budget peers by 4 or 5 pm.\nBy the numbers:\n\n\nShow the code\n(delays.groupby('Airline').count()[['CRSDepTime']] / flights.groupby('Airline').count()[['CRSDepTime']] * 100).sort_values('CRSDepTime', ascending=False).rename({'CRSDepTime':'% Flights Significantly Delayed'}, axis=1)\n\n\n\n    \n      \n\n\n\n\n\n\n% Flights Significantly Delayed\n\n\nAirline\n\n\n\n\n\nJetBlue Airways\n22.078514\n\n\nFrontier Airlines Inc.\n21.896298\n\n\nAllegiant Air\n18.991785\n\n\nSpirit Air Lines\n16.958591\n\n\nSouthwest Airlines Co.\n15.292280\n\n\nAmerican Airlines Inc.\n14.529106\n\n\nMesa Airlines Inc.\n13.838768\n\n\nUnited Air Lines Inc.\n12.994220\n\n\nHawaiian Airlines Inc.\n11.101448\n\n\nPSA Airlines Inc.\n11.007379\n\n\nDelta Air Lines Inc.\n10.671968\n\n\nSkyWest Airlines Inc.\n10.629637\n\n\nAlaska Airlines Inc.\n10.458504\n\n\nRepublic Airline\n10.361243\n\n\nEndeavor Air Inc.\n9.843149\n\n\nEnvoy Air\n9.762695\n\n\nHorizon Air\n8.873647\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\nSince starting college, I’ve traveled quite regularly on planes between my hometown in Southern California and my university in the San Francisco Bay Area. I was drawn to this topic to get a better sense of which airline and airport combination offered me the fewest overall delays. As a college student with a constrained budget, I often take Southwest to and from school due to its cheap fares and customer service..\nSouthern California common wisdom dictates that one should fly out of Burbank (BUR) whenever possible to avoid the hassle of traveling to and out of Los Angeles (LAX).. Coming to the Bay Area for college, I expected Oakland (OAK) and San Francisco (SFO) to share a similar dynamic. add another sentence here for a better flowHowever, I was surprised to hear that some of my friends preferred SFO over OAK. In this section, we will determine whether SFO is better than OAK and BUR is better than LAX.\nI follow a similar approach as before but select only flight records that contain our desired airports.\n\n\nShow the code\ndef compare_airports(airlines, airports, delays, flights):\n    \"\"\" Returns a DataFrame with delay rates by airport and airline. Indices follow the format 'airline airport.'\n    airlines' and airports must be lists. delays and flights are DataFrames as created above.\n    \"\"\"\n    proportions = pd.DataFrame()\n    for airline in airlines:\n        for origin in airports:\n            proportions[airline + \" \" + origin] = airline_delay_frequencies(airline,\n                            delays[delays['Origin'] == origin], flights[flights['Origin'] == origin])\n    return proportions\n\n# Construct a bar plot of flight delay percentage according to time\na = sns.lineplot(data=compare_airports(['Southwest Airlines Co.'], ['LAX', 'BUR', 'OAK', 'SFO'], delays_ma, flights_ma))\nplt.xlabel(\"Scheduled Hour of Flight Departure\")\nplt.ylabel(\"Percentage\")\nplt.title(\"Percent of flights delayed by airport, airline\")\nplt.xticks(range(0,24,4), ['12am','4am','8am','12pm','4pm','8pm'])\nsns.move_legend(a, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n\n\n\n\n\n\n\nWe see that BUR has fewer delays than LAX, OAK, and SFO. Burbank is better than LAX, but Oakland and SFO are about the same. Maybe my friends’ preference for SFO compared to OAK stems from something else, but clearly delays don’t play a major role in that decision."
  },
  {
    "objectID": "posts/Flights-technical.html#downloading-the-data",
    "href": "posts/Flights-technical.html#downloading-the-data",
    "title": "Everything to Know About Flight Delays",
    "section": "",
    "text": "The best resource to investigate the regularity of flights within the US is the Bureau of Transportation Statistics. They have a website that hosts the data, which is publicly available. The data is available by month, meaning that I had to manually request data for the 14 months I was interested in.\nWe can automate this download with the requests library of Python, setting START_YEAR to the beginning year of the period of interest and END_YEAR to the end year (exclusive).\n\n\nShow the code\nSTART_YEAR = 2022\nEND_YEAR = 2024\n\n\n\n\nShow the code\nos.mkdir(\"data\")\n\n\n\n\nShow the code\nfor i in range(START_YEAR, END_YEAR):\n    for j in range(1,13):\n        # We set verify to False because an SSL cert error gets thrown otherwise for some reason.\n        # For some reason, creating a pool manager as described on the Certifi documentation\n        # throws an SSL connection error. It could be an issue with Google Colab, which I am using to\n        # work with this notebook.\n        r = requests.get(f\"https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{i}_{j}.zip\", verify=False)\n        with open(f\"data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{i}_{j}.zip\", \"wb\") as fd:\n            fd.write(r.content)\n\n\nAt this point, we have a bunch of zip files, one for each month of flights. We’ll unzip them one-by-one.\n\n\nShow the code\n#Construct an array to hold our zip filepaths\nzips = []\nfor root, directories, filenames in os.walk('data'):\n    for directory in directories:\n         dpath = os.path.join(root, directory)\n\n    for filename in filenames:\n        fpath = os.path.join(root,filename)\n        if fpath[-3:] == 'zip':\n            zips.append(fpath)\n\nfor zip_path in zips:\n    try:\n        my_zip = zipfile.ZipFile(zip_path, 'r')\n        my_zip.extractall('data')\n    except zipfile.BadZipFile:\n        continue"
  },
  {
    "objectID": "posts/Flights-technical.html#reading-in-and-cleaning-data",
    "href": "posts/Flights-technical.html#reading-in-and-cleaning-data",
    "title": "Everything to Know About Flight Delays",
    "section": "",
    "text": "Now, we read the data in and clean it. We start out by reading in an arbitrary month to get a DataFrame with the correct columns, then loop through each month’s flight records (stored in a csv). We add each month’s flight records to our flights DataFrame; at this point, we are done loading data for this project.\nCleaning the DataFrames involves several steps. To start, we read in this HTML table from the Bureau of Transportation Statistics as a DataFrame. This table matches airlines with abbreviations. To aid legibility, we replace the abbreviations contained in our flights DataFrame with the airlines’ full names.\n\n\nShow the code\nimport re\n\n#Obtains DataFrame with correct columns\nflights = pd.DataFrame(columns=pd.read_csv(\"data/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_5.csv\").columns)\n\n#Concatenates inputs to flights DataFrame\nfor root, directories, filenames in os.walk('data'):\n    for filename in filenames:\n        fpath = os.path.join(root,filename)\n        if (fpath[-3:] == 'csv') & (fpath[5:8] == 'On_'):\n            a = pd.read_csv(fpath)\n\n            # Filter out unneeded rows (only keep things relevant to flight origin, airline, and departure delay)\n            a = a.filter(['CRSDepTime', 'DepDelayMinutes', 'Reporting_Airline', 'TaxiOut', 'Origin', 'Flight_Number_Reporting_Airline'], axis=1)\n\n            # Concatenate month table to flights\n            print(f\"  {fpath}\")\n            flights = pd.concat([flights, a], ignore_index=True, join='inner')\n\n# Obtains a DataFrame version of the HTML table on the BTS Airline Codes webpage\ncodes = pd.read_html(\"https://www.bts.gov/topics/airlines-and-airports/airline-codes\")[0]\nflights = flights.merge(right=codes, left_on='Reporting_Airline', right_on='Code', suffixes=['',''])#.drop([\"Reporting_Airline\", 'Code'], axis=1)\n\n\nWe now make a copy of flights called delays. We can now safely transform our data and keep the original flights available for later analysis. We’ll also regularize our time (converting the HHMM format to HH.MM, where MM is out of 100 instead of 60).\n\n\nShow the code\n# Makes a deep copy of flights called delays for transformation in this section. Filter out on-time flights.\nflights['CRSDepTime'] = flights['CRSDepTime'] % 100 * 5/3 * .01 + flights['CRSDepTime'] // 100\nflights['delay'] = flights['DepDelayMinutes'] &gt; 0\ndelays = flights.copy(deep=True)[flights['DepDelayMinutes'] &gt; 0]"
  },
  {
    "objectID": "posts/Flights-technical.html#plot-of-flight-delays-vs.-departure-time",
    "href": "posts/Flights-technical.html#plot-of-flight-delays-vs.-departure-time",
    "title": "Everything to Know About Flight Delays",
    "section": "",
    "text": "Let’s take a precursory view of flight delays in the aggregate and see how they correlate with departure time.\n\n\nShow the code\nsns.set_theme()\n\nf, ax = plt.subplots(figsize=(7, 7), sharex=True, sharey=True)\na = sns.histplot(data=delays, x='CRSDepTime', y='DepDelayMinutes', bins=[12,600],\n                 cmap=sns.color_palette(\"flare_r\", as_cmap=True), cbar=True, cbar_kws={'label': 'Quantity of delays'})\nsns.kdeplot(data=delays.sample(1000), x='CRSDepTime', y='DepDelayMinutes', levels=5, color=\"w\")\n\nplt.title('Density of delays by departure time, ' + str(START_YEAR) + \"-\" + str(END_YEAR - 1))\nplt.xlabel('Departure time')\nplt.ylabel('Departure delay, in minutes')\nplt.xlim((0, 24))\nplt.ylim(bottom=0, top=90)\nplt.xticks(range(0,24,4), ['12am','4am','8am','12pm','4pm','8pm'])\n\n\n([&lt;matplotlib.axis.XTick at 0x7fa796f36290&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa796f35c00&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7d57ca740&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7894ad390&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7894ac340&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7894ae500&gt;],\n [Text(0, 0, '12am'),\n  Text(4, 0, '4am'),\n  Text(8, 0, '8am'),\n  Text(12, 0, '12pm'),\n  Text(16, 0, '4pm'),\n  Text(20, 0, '8pm')])\n\n\n\n\n\n\n\n\n\nWe see that delays overall are densest between 2pm and 6pm, but the most common delay is less than 10 minutes and occurs between 10am and 12pm. Given that the most common delay is minimal, I wanted to take a better look. We’ll look at significant delays, which I will define to be greater than or equal to 30 minutes in length."
  },
  {
    "objectID": "posts/Flights-technical.html#plot-of-significant-and-nonsignificant-delays-vs.-departure",
    "href": "posts/Flights-technical.html#plot-of-significant-and-nonsignificant-delays-vs.-departure",
    "title": "Everything to Know About Flight Delays",
    "section": "",
    "text": "Show the code\ndef delay_significance(x):\n    \"\"\"Categorizes delays into one of three categories.\"\"\"\n    if x == 0:\n        return \"On Time\"\n    elif x &lt; 30 and x &gt; 0:\n        return \"Slight Delay\"\n    else:\n        return \"Significant Delay\"\n\n#Creates labels for hue sorting on histogram.\nflights['Delay Status'] = flights['DepDelayMinutes'].apply(delay_significance)\n\n\n\n\nShow the code\nsns.histplot(flights, x='CRSDepTime', kde=True, hue='Delay Status',\n             binwidth=1, kde_kws={'bw_adjust': 3}).set(xlabel='Departure time', title='Flights from '\n                             + str(START_YEAR) + ' to ' + str(END_YEAR - 1) + ', by hour')\n\nplt.xlim(0,24)\nplt.xticks(range(0,24,4), ['12am','4am','8am','12pm','4pm','8pm'])\n\n\n([&lt;matplotlib.axis.XTick at 0x7fa7849715d0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7849715a0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa784972a40&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7858f0b80&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7858f1630&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7858f20e0&gt;],\n [Text(0, 0, '12am'),\n  Text(4, 0, '4am'),\n  Text(8, 0, '8am'),\n  Text(12, 0, '12pm'),\n  Text(16, 0, '4pm'),\n  Text(20, 0, '8pm')])\n\n\n\n\n\n\n\n\n\nFrom the chart, we see that both slight and significant delays increase as the day goes on. Significant delays start low at 6am and steadily increase, peaking at 6pm, while slight delays seem to plateau around 10am."
  },
  {
    "objectID": "posts/Flights-technical.html#delays-by-airline",
    "href": "posts/Flights-technical.html#delays-by-airline",
    "title": "Everything to Know About Flight Delays",
    "section": "",
    "text": "Now, let’s look at delays by airline. We merge wholly-owned subsidiaries with their parent company (a complete list of wholly-owned subsidiary airlines in North America can be found on Wikipedia), as people do not often see their brand nor purchase from them. Some airlines, such as Republic Airline, are regional airlines that fly under multiple airline names; our data does not represent whose banner they fly under for a given flight, making it impossible for us to merge their flights with their contracted carrier.\n\n\nShow the code\n# Adds new departure hour columns to flights and delays for easier charting\nflights['CRSDepHour'], delays['CRSDepHour'] = flights['CRSDepTime'] // 1,  delays['CRSDepTime'] // 1\n\n#Selects only for significant delays\ndelays = delays[delays['DepDelayMinutes'] &gt;= 30]\n\n# Merges wholly-owned subsidiary airlines with their parent companies\ndelays_ma = delays.replace(to_replace={\"Airline\": {\"Horizon Air\":\"Alaska Airlines Inc.\", \"Envoy Air\":\"American Airlines Inc.\", \"PSA Airlines Inc.\":\"American Airlines Inc.\", \"Endeavor Air Inc.\": \"Delta Air Lines Inc.\"}})\nflights_ma = flights.replace(to_replace={\"Airline\": {\"Horizon Air\":\"Alaska Airlines Inc.\", \"Envoy Air\":\"American Airlines Inc.\", \"PSA Airlines Inc.\":\"American Airlines Inc.\", \"Endeavor Air Inc.\": \"Delta Air Lines Inc.\"}})\n\ndef airline_delay_frequencies(airline, delays, flights):\n    \"\"\"Returns delay proportions, grouped by departure hour and departure time.\"\"\"\n    total_delays = delays[delays['Airline'] == airline].groupby('CRSDepHour')['CRSDepTime'].count()\n    total_flights = flights[flights['Airline'] == airline].groupby('CRSDepHour')['CRSDepTime'].count()\n    return (total_delays / total_flights).reindex(np.arange(24), fill_value=0) * 100\n\n# Accumulates multiple airline Series into a DataFrame for line plot\nairlines = ['American Airlines Inc.', #'SkyWest Airlines Inc.',\n       'Alaska Airlines Inc.', 'United Air Lines Inc.',\n       'Delta Air Lines Inc.', 'Frontier Airlines Inc.', #'Allegiant Air',\n       #'Hawaiian Airlines Inc.',\n        'Spirit Air Lines',\n       'Southwest Airlines Co.', #'Mesa Airlines Inc.', 'Republic Airline',\n       'JetBlue Airways']\nproportions = pd.DataFrame()\nfor airline in airlines:\n    proportions[airline] = airline_delay_frequencies(airline, delays_ma, flights_ma)\n\n\n\n\nShow the code\n# Constructs a bar plot of flight delay percentage according to time\na = sns.lineplot(data=proportions, palette=\"tab10\", linewidth=2.5)\nplt.xlabel(\"Scheduled Hour of Flight Departure\")\nplt.ylabel(\"Percentage\")\nplt.title(\"Percent of flights significantly delayed, by time of day\")\nplt.xticks(range(0,24,4), ['12am','4am','8am','12pm','4pm','8pm'])\nplt.ylim(0, 40)\nsns.move_legend(a, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n\n\n\n\n\n\n\nRegardless of airline, the chance of significant delay is lowest in the early hours of the morning, regardless of airline. From there, the probability of delay steadily increases and peaks in the evening. Then there is a dip around midnight, with delays skyrocketing in the wee hours of the morning. Many of these flights are budget airline red-eyes from Alaska and Puerto Rico that occur at the end of an airline’s work day, which can be later than 2am in those areas. Flying at the end of the work day increases the chance of delay since planes fly multiple flights in a day, and any delay in an earlier flight can mess up the traffic control schedule for all later flights. Additionally, if delayed for too long, crew duty hours can also exceed the limit. This happened in Southwest’s meltdow last year, where Southwest had insufficient replacement crews and misallocation of planes, leading to mass flight cancellations.\nThe budget airlines (Frontier, Spirit, Southwest, and JetBlue) all have higher rates of delay throughout the day according to this dataset, with the exception of Southwest, which has a delay rate comparable with the non-budget airlines in the morning. However, by 12pm, Southwest’s significant delay rate grows past that of the non-budget airlines and joins its budget peers by 4 or 5 pm.\nBy the numbers:\n\n\nShow the code\n(delays.groupby('Airline').count()[['CRSDepTime']] / flights.groupby('Airline').count()[['CRSDepTime']] * 100).sort_values('CRSDepTime', ascending=False).rename({'CRSDepTime':'% Flights Significantly Delayed'}, axis=1)\n\n\n\n    \n      \n\n\n\n\n\n\n% Flights Significantly Delayed\n\n\nAirline\n\n\n\n\n\nJetBlue Airways\n22.078514\n\n\nFrontier Airlines Inc.\n21.896298\n\n\nAllegiant Air\n18.991785\n\n\nSpirit Air Lines\n16.958591\n\n\nSouthwest Airlines Co.\n15.292280\n\n\nAmerican Airlines Inc.\n14.529106\n\n\nMesa Airlines Inc.\n13.838768\n\n\nUnited Air Lines Inc.\n12.994220\n\n\nHawaiian Airlines Inc.\n11.101448\n\n\nPSA Airlines Inc.\n11.007379\n\n\nDelta Air Lines Inc.\n10.671968\n\n\nSkyWest Airlines Inc.\n10.629637\n\n\nAlaska Airlines Inc.\n10.458504\n\n\nRepublic Airline\n10.361243\n\n\nEndeavor Air Inc.\n9.843149\n\n\nEnvoy Air\n9.762695\n\n\nHorizon Air\n8.873647"
  },
  {
    "objectID": "posts/Flights-technical.html#applications-lax-and-bur-and-sfo-and-oak",
    "href": "posts/Flights-technical.html#applications-lax-and-bur-and-sfo-and-oak",
    "title": "Everything to Know About Flight Delays",
    "section": "",
    "text": "Since starting college, I’ve traveled quite regularly on planes between my hometown in Southern California and my university in the San Francisco Bay Area. I was drawn to this topic to get a better sense of which airline and airport combination offered me the fewest overall delays. As a college student with a constrained budget, I often take Southwest to and from school due to its cheap fares and customer service..\nSouthern California common wisdom dictates that one should fly out of Burbank (BUR) whenever possible to avoid the hassle of traveling to and out of Los Angeles (LAX).. Coming to the Bay Area for college, I expected Oakland (OAK) and San Francisco (SFO) to share a similar dynamic. add another sentence here for a better flowHowever, I was surprised to hear that some of my friends preferred SFO over OAK. In this section, we will determine whether SFO is better than OAK and BUR is better than LAX.\nI follow a similar approach as before but select only flight records that contain our desired airports.\n\n\nShow the code\ndef compare_airports(airlines, airports, delays, flights):\n    \"\"\" Returns a DataFrame with delay rates by airport and airline. Indices follow the format 'airline airport.'\n    airlines' and airports must be lists. delays and flights are DataFrames as created above.\n    \"\"\"\n    proportions = pd.DataFrame()\n    for airline in airlines:\n        for origin in airports:\n            proportions[airline + \" \" + origin] = airline_delay_frequencies(airline,\n                            delays[delays['Origin'] == origin], flights[flights['Origin'] == origin])\n    return proportions\n\n# Construct a bar plot of flight delay percentage according to time\na = sns.lineplot(data=compare_airports(['Southwest Airlines Co.'], ['LAX', 'BUR', 'OAK', 'SFO'], delays_ma, flights_ma))\nplt.xlabel(\"Scheduled Hour of Flight Departure\")\nplt.ylabel(\"Percentage\")\nplt.title(\"Percent of flights delayed by airport, airline\")\nplt.xticks(range(0,24,4), ['12am','4am','8am','12pm','4pm','8pm'])\nsns.move_legend(a, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n\n\n\n\n\n\n\nWe see that BUR has fewer delays than LAX, OAK, and SFO. Burbank is better than LAX, but Oakland and SFO are about the same. Maybe my friends’ preference for SFO compared to OAK stems from something else, but clearly delays don’t play a major role in that decision."
  }
]