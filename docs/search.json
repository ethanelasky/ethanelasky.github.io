[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Ethan Elasky’s Blog",
    "section": "",
    "text": "Notes on Taiwanese Indie Music\n\n\n\n\n\nFresh songs from across the planet\n\n\n\n\n\n\nDec 29, 2023\n\n\nEthan Elasky\n\n\n\n\n\n\n  \n\n\n\n\nAmazon Photos collects your photo data – you can disable this\n\n\n\n\n\nDon’t become Amazon’s training data\n\n\n\n\n\n\nNov 15, 2023\n\n\nEthan Elasky\n\n\n\n\n\n\n  \n\n\n\n\nEverything to Know About Flight Delays\n\n\n\n\n\nAnalyzing post-Covid airline records with Python\n\n\n\n\n\n\nJul 30, 2023\n\n\nEthan Elasky\n\n\n\n\n\n\n  \n\n\n\n\nEverything to Know About Flight Delays (no code)\n\n\n\n\n\nAnalyzing post-Covid airline records\n\n\n\n\n\n\nJul 30, 2023\n\n\nEthan Elasky\n\n\n\n\n\n\n  \n\n\n\n\nImproving Argument Extensions in Lincoln-Douglas debate\n\n\n\n\n\nHow to explain your arguments clearly\n\n\n\n\n\n\nJul 24, 2020\n\n\nEthan Elasky\n\n\n\n\n\n\n  \n\n\n\n\nAvoiding overfitting in Lincoln-Douglas debate practice\n\n\n\n\n\nKeeping your skills robust against new arguments\n\n\n\n\n\n\nMay 28, 2020\n\n\nEthan Elasky\n\n\n\n\n\n\n  \n\n\n\n\nEmbedded Clash - a Primer\n\n\n\n\n\nGetting through your flow faster than ever\n\n\n\n\n\n\nMay 28, 2020\n\n\nEthan Elasky\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/improving-argument-extensions.html",
    "href": "posts/improving-argument-extensions.html",
    "title": "Improving Argument Extensions in Lincoln-Douglas debate",
    "section": "",
    "text": "It’s important to know how to extend arguments correctly. If you fail to give a clear understanding of your arguments to the judge, it’s unlikely they’ll vote for you.\nThe first thing that you should do is explain the thesis or impact of your argument. In my opinion, if you’re extending an individual argument (i.e. a single card), you should always just explain the thesis of it. If you’re extending a position, the choice to explain your overarching thesis or terminal impact should depend on the off/on-case argument you’re going for.\nI’ll list what I think the best top-level overview explanation is for each:\n\nCritique: theory of power or impact description (dependent on critique).\nCounterplan: clear explanation of what each plank does.\nDisadvantage: terminal impact description and comparison.\nTopicality shell: overarching view of the model that the aff should defend. Something similar to “Our interpretation is that the aff should defend ______________.” Then, get into impact comparison.\nOffensive case turn: if it has an external impact, treat it as a disadvantage. If it only straight turns one of the aff’s internal links, explain the thesis of the straight turn.\nDefensive case turn: thesis.\n\nThis list won’t work perfectly for all positions, but it can serve as a guiding tool for when you’re unsure.\nA sample disadvantage extension for the 2020 Base DA could sound something like “Trump lash-out goes nuclear – strikes on Chinese mainland spark catastrophic escalation because the CCP will escalate to de-escalate, thinking that they’ll scare Trump into giving up.” This is a good length for any speech starter; it makes a coherent argument in ~25 words, and it clearly paints a picture for the judge of what your argument is.\nThe second thing to do is frame your argument/position. When I say framing, I mean explaining to the judge why your position should be preferred over your opponent’s. if you’re extending a full position, this can include weighing (think magnitude, timeframe, probability), turns case, or important issues to deal with in the overview. If you’re extending a single piece of evidence, you still need reasons to prefer your cards to your opponent’s—examples include depth/breadth/type/specificity of warrants, author/publication qualifications, or recency. And it’s also important to create clear judge instruction that describes why your comparisons should matter more than the other side’s (i.e. Bostrom, etc).\nThere is a third type of argument resolution that I call judge instruction. It’s important to resolve issues in the judge’s mind and instruct them on how to evaluate the round because if you don’t, you may not be happy with the judge’s defaults. I can think of a few instances that illustrate the importance of this. First, in a debate where the 2NR is a process counterplan that solves the aff with a very questionable net benefit, how should the judge evaluate the net benefit? Is it best for the judge to vote on “any risk” of the disadvantage, or should they hold the negative to the bar of presenting a “complete” argument in the fullest sense of the word? Second, in a policy aff vs K debate, how should the judge evaluate the perm? Is the perm an advocacy or a test of competition? In what cases does the aff get the perm? To answer the last question, it might be best for the negative to say that the aff only gets the permutation if they prove “no link,” but the aff might be better off if the judge thought that the permutation can “shield” some of the link to the aff. It is these framing questions that are mostly unanswered in debates at the 3-3 level and lead to many split decisions in elimination rounds. If you’re interested in reading more about this, Scott Phillips delves into this in this post on HS Impact.\nThis isn’t to say every argument ever needs multiple framing issues; you sorta have to collapse more so you have time to resolve the most important parts of the debate in your favor.\nNow that I’m done with my rant about the importance of framing and judge instruction, we’ll deal with the last part of any argument extension: the line by line.\nThis is the simplest part of extending an argument; you just need to answer the other side’s arguments. You should extend important parts of your argument that you didn’t explain above, if any. I’ve seen many high-level critique debaters extend their link(s) on the permutation, and most for whom politics is the disadvantage du jour always extend uniqueness on the first non-unique argument made by the 2AC/1AR.\nTo summarize, there are three parts to extending any argument: first, a top-level explanation that contextualizes your argument for the judge; second, framing issue(s) that shows the judge how to evaluate the debate in your favor; third, a line-by-line refutation that proves to the judge that your argument doesn’t have glaring holes in it. This is something worth practicing, because regardless of the content of your position, you need to be able to competently explain your position/evidence and compare it/them to the other side’s to win."
  },
  {
    "objectID": "posts/Flights-technical.html",
    "href": "posts/Flights-technical.html",
    "title": "Everything to Know About Flight Delays",
    "section": "",
    "text": "This is the technical version of an article that analyzes flight information from January 2022 to February 2023 (the latest available as of today). Of great interest are flight delays.\n\n\nCode\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport zipfile\nimport os\nimport requests\nfrom pathlib import Path\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n\n\nThe best resource to investigate the regularity of flights within the US is the Bureau of Transportation Statistics. They have a website that hosts the data, which is publicly available. The data is available by month, meaning that I had to manually request data for the 14 months I was interested in.\nWe can automate this download with the requests library of Python, setting START_YEAR to the beginning year of the period of interest and END_YEAR to the end year (exclusive).\n\n\nCode\nSTART_YEAR = 2022\nEND_YEAR = 2024\n\n\n\n\nCode\nos.mkdir(\"data\")\n\n\n\n\nCode\nfor i in range(START_YEAR, END_YEAR):\n    for j in range(1,13):\n        # We set verify to False because an SSL cert error gets thrown otherwise for some reason.\n        # For some reason, creating a pool manager as described on the Certifi documentation\n        # throws an SSL connection error. It could be an issue with Google Colab, which I am using to\n        # work with this notebook.\n        r = requests.get(f\"https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{i}_{j}.zip\", verify=False)\n        with open(f\"data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{i}_{j}.zip\", \"wb\") as fd:\n            fd.write(r.content)\n\n\nAt this point, we have a bunch of zip files, one for each month of flights. We’ll unzip them one-by-one.\n\n\nCode\n#Construct an array to hold our zip filepaths\nzips = []\nfor root, directories, filenames in os.walk('data'):\n    for directory in directories:\n         dpath = os.path.join(root, directory)\n\n    for filename in filenames:\n        fpath = os.path.join(root,filename)\n        if fpath[-3:] == 'zip':\n            zips.append(fpath)\n\nfor zip_path in zips:\n    try:\n        my_zip = zipfile.ZipFile(zip_path, 'r')\n        my_zip.extractall('data')\n    except zipfile.BadZipFile:\n        continue\n\n\n\n\n\nNow, we read the data in and clean it. We start out by reading in an arbitrary month to get a DataFrame with the correct columns, then loop through each month’s flight records (stored in a csv). We add each month’s flight records to our flights DataFrame; at this point, we are done loading data for this project.\nCleaning the DataFrames involves several steps. To start, we read in this HTML table from the Bureau of Transportation Statistics as a DataFrame. This table matches airlines with abbreviations. To aid legibility, we replace the abbreviations contained in our flights DataFrame with the airlines’ full names.\n\n\nCode\nimport re\n\n#Obtains DataFrame with correct columns\nflights = pd.DataFrame(columns=pd.read_csv(\"data/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_5.csv\").columns)\n\n#Concatenates inputs to flights DataFrame\nfor root, directories, filenames in os.walk('data'):\n    for filename in filenames:\n        fpath = os.path.join(root,filename)\n        if (fpath[-3:] == 'csv') & (fpath[5:8] == 'On_'):\n            a = pd.read_csv(fpath)\n\n            # Filter out unneeded rows (only keep things relevant to flight origin, airline, and departure delay)\n            a = a.filter(['CRSDepTime', 'DepDelayMinutes', 'Reporting_Airline', 'TaxiOut', 'Origin', 'Flight_Number_Reporting_Airline'], axis=1)\n\n            # Concatenate month table to flights\n            print(f\"  {fpath}\")\n            flights = pd.concat([flights, a], ignore_index=True, join='inner')\n\n# Obtains a DataFrame version of the HTML table on the BTS Airline Codes webpage\ncodes = pd.read_html(\"https://www.bts.gov/topics/airlines-and-airports/airline-codes\")[0]\nflights = flights.merge(right=codes, left_on='Reporting_Airline', right_on='Code', suffixes=['',''])#.drop([\"Reporting_Airline\", 'Code'], axis=1)\n\n\nWe now make a copy of flights called delays. We can now safely transform our data and keep the original flights available for later analysis. We’ll also regularize our time (converting the HHMM format to HH.MM, where MM is out of 100 instead of 60).\n\n\nCode\n# Makes a deep copy of flights called delays for transformation in this section. Filter out on-time flights.\nflights['CRSDepTime'] = flights['CRSDepTime'] % 100 * 5/3 * .01 + flights['CRSDepTime'] // 100\nflights['delay'] = flights['DepDelayMinutes'] &gt; 0\ndelays = flights.copy(deep=True)[flights['DepDelayMinutes'] &gt; 0]\n\n\n\n\n\nLet’s take a precursory view of flight delays in the aggregate and see how they correlate with departure time.\n\n\nCode\nsns.set_theme()\n\nf, ax = plt.subplots(figsize=(7, 7), sharex=True, sharey=True)\na = sns.histplot(data=delays, x='CRSDepTime', y='DepDelayMinutes', bins=[12,600],\n                 cmap=sns.color_palette(\"flare_r\", as_cmap=True), cbar=True, cbar_kws={'label': 'Quantity of delays'})\nsns.kdeplot(data=delays.sample(1000), x='CRSDepTime', y='DepDelayMinutes', levels=5, color=\"w\")\n\nplt.title('Density of delays by departure time, ' + str(START_YEAR) + \"-\" + str(END_YEAR - 1))\nplt.xlabel('Departure time')\nplt.ylabel('Departure delay, in minutes')\nplt.xlim((0, 24))\nplt.ylim(bottom=0, top=90)\nplt.xticks(range(0,24,4), ['12am','4am','8am','12pm','4pm','8pm'])\n\n\n([&lt;matplotlib.axis.XTick at 0x7fa796f36290&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa796f35c00&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7d57ca740&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7894ad390&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7894ac340&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7894ae500&gt;],\n [Text(0, 0, '12am'),\n  Text(4, 0, '4am'),\n  Text(8, 0, '8am'),\n  Text(12, 0, '12pm'),\n  Text(16, 0, '4pm'),\n  Text(20, 0, '8pm')])\n\n\n\n\n\nWe see that delays overall are densest between 2pm and 6pm, but the most common delay is less than 10 minutes and occurs between 10am and 12pm. Given that the most common delay is minimal, I wanted to take a better look. We’ll look at significant delays, which I will define to be greater than or equal to 30 minutes in length.\n\n\n\n\n\nCode\ndef delay_significance(x):\n    \"\"\"Categorizes delays into one of three categories.\"\"\"\n    if x == 0:\n        return \"On Time\"\n    elif x &lt; 30 and x &gt; 0:\n        return \"Slight Delay\"\n    else:\n        return \"Significant Delay\"\n\n#Creates labels for hue sorting on histogram.\nflights['Delay Status'] = flights['DepDelayMinutes'].apply(delay_significance)\n\n\n\n\nCode\nsns.histplot(flights, x='CRSDepTime', kde=True, hue='Delay Status',\n             binwidth=1, kde_kws={'bw_adjust': 3}).set(xlabel='Departure time', title='Flights from '\n                             + str(START_YEAR) + ' to ' + str(END_YEAR - 1) + ', by hour')\n\nplt.xlim(0,24)\nplt.xticks(range(0,24,4), ['12am','4am','8am','12pm','4pm','8pm'])\n\n\n([&lt;matplotlib.axis.XTick at 0x7fa7849715d0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7849715a0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa784972a40&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7858f0b80&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7858f1630&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7858f20e0&gt;],\n [Text(0, 0, '12am'),\n  Text(4, 0, '4am'),\n  Text(8, 0, '8am'),\n  Text(12, 0, '12pm'),\n  Text(16, 0, '4pm'),\n  Text(20, 0, '8pm')])\n\n\n\n\n\nFrom the chart, we see that both slight and significant delays increase as the day goes on. Significant delays start low at 6am and steadily increase, peaking at 6pm, while slight delays seem to plateau around 10am.\n\n\n\nNow, let’s look at delays by airline. We merge wholly-owned subsidiaries with their parent company (a complete list of wholly-owned subsidiary airlines in North America can be found on Wikipedia), as people do not often see their brand nor purchase from them. Some airlines, such as Republic Airline, are regional airlines that fly under multiple airline names; our data does not represent whose banner they fly under for a given flight, making it impossible for us to merge their flights with their contracted carrier.\n\n\nCode\n# Adds new departure hour columns to flights and delays for easier charting\nflights['CRSDepHour'], delays['CRSDepHour'] = flights['CRSDepTime'] // 1,  delays['CRSDepTime'] // 1\n\n#Selects only for significant delays\ndelays = delays[delays['DepDelayMinutes'] &gt;= 30]\n\n# Merges wholly-owned subsidiary airlines with their parent companies\ndelays_ma = delays.replace(to_replace={\"Airline\": {\"Horizon Air\":\"Alaska Airlines Inc.\", \"Envoy Air\":\"American Airlines Inc.\", \"PSA Airlines Inc.\":\"American Airlines Inc.\", \"Endeavor Air Inc.\": \"Delta Air Lines Inc.\"}})\nflights_ma = flights.replace(to_replace={\"Airline\": {\"Horizon Air\":\"Alaska Airlines Inc.\", \"Envoy Air\":\"American Airlines Inc.\", \"PSA Airlines Inc.\":\"American Airlines Inc.\", \"Endeavor Air Inc.\": \"Delta Air Lines Inc.\"}})\n\ndef airline_delay_frequencies(airline, delays, flights):\n    \"\"\"Returns delay proportions, grouped by departure hour and departure time.\"\"\"\n    total_delays = delays[delays['Airline'] == airline].groupby('CRSDepHour')['CRSDepTime'].count()\n    total_flights = flights[flights['Airline'] == airline].groupby('CRSDepHour')['CRSDepTime'].count()\n    return (total_delays / total_flights).reindex(np.arange(24), fill_value=0) * 100\n\n# Accumulates multiple airline Series into a DataFrame for line plot\nairlines = ['American Airlines Inc.', #'SkyWest Airlines Inc.',\n       'Alaska Airlines Inc.', 'United Air Lines Inc.',\n       'Delta Air Lines Inc.', 'Frontier Airlines Inc.', #'Allegiant Air',\n       #'Hawaiian Airlines Inc.',\n        'Spirit Air Lines',\n       'Southwest Airlines Co.', #'Mesa Airlines Inc.', 'Republic Airline',\n       'JetBlue Airways']\nproportions = pd.DataFrame()\nfor airline in airlines:\n    proportions[airline] = airline_delay_frequencies(airline, delays_ma, flights_ma)\n\n\n\n\nCode\n# Constructs a bar plot of flight delay percentage according to time\na = sns.lineplot(data=proportions, palette=\"tab10\", linewidth=2.5)\nplt.xlabel(\"Scheduled Hour of Flight Departure\")\nplt.ylabel(\"Percentage\")\nplt.title(\"Percent of flights significantly delayed, by time of day\")\nplt.xticks(range(0,24,4), ['12am','4am','8am','12pm','4pm','8pm'])\nplt.ylim(0, 40)\nsns.move_legend(a, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n\n\n\nRegardless of airline, the chance of significant delay is lowest in the early hours of the morning, regardless of airline. From there, the probability of delay steadily increases and peaks in the evening. Then there is a dip around midnight, with delays skyrocketing in the wee hours of the morning. Many of these flights are budget airline red-eyes from Alaska and Puerto Rico that occur at the end of an airline’s work day, which can be later than 2am in those areas. Flying at the end of the work day increases the chance of delay since planes fly multiple flights in a day, and any delay in an earlier flight can mess up the traffic control schedule for all later flights. Additionally, if delayed for too long, crew duty hours can also exceed the limit. This happened in Southwest’s meltdow last year, where Southwest had insufficient replacement crews and misallocation of planes, leading to mass flight cancellations.\nThe budget airlines (Frontier, Spirit, Southwest, and JetBlue) all have higher rates of delay throughout the day according to this dataset, with the exception of Southwest, which has a delay rate comparable with the non-budget airlines in the morning. However, by 12pm, Southwest’s significant delay rate grows past that of the non-budget airlines and joins its budget peers by 4 or 5 pm.\nBy the numbers:\n\n\nCode\n(delays.groupby('Airline').count()[['CRSDepTime']] / flights.groupby('Airline').count()[['CRSDepTime']] * 100).sort_values('CRSDepTime', ascending=False).rename({'CRSDepTime':'% Flights Significantly Delayed'}, axis=1)\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\n% Flights Significantly Delayed\n\n\nAirline\n\n\n\n\n\nJetBlue Airways\n22.078514\n\n\nFrontier Airlines Inc.\n21.896298\n\n\nAllegiant Air\n18.991785\n\n\nSpirit Air Lines\n16.958591\n\n\nSouthwest Airlines Co.\n15.292280\n\n\nAmerican Airlines Inc.\n14.529106\n\n\nMesa Airlines Inc.\n13.838768\n\n\nUnited Air Lines Inc.\n12.994220\n\n\nHawaiian Airlines Inc.\n11.101448\n\n\nPSA Airlines Inc.\n11.007379\n\n\nDelta Air Lines Inc.\n10.671968\n\n\nSkyWest Airlines Inc.\n10.629637\n\n\nAlaska Airlines Inc.\n10.458504\n\n\nRepublic Airline\n10.361243\n\n\nEndeavor Air Inc.\n9.843149\n\n\nEnvoy Air\n9.762695\n\n\nHorizon Air\n8.873647\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\nSince starting college, I’ve traveled quite regularly on planes between my hometown in Southern California and my university in the San Francisco Bay Area. I was drawn to this topic to get a better sense of which airline and airport combination offered me the fewest overall delays. As a college student with a constrained budget, I often take Southwest to and from school due to its cheap fares and customer service..\nSouthern California common wisdom dictates that one should fly out of Burbank (BUR) whenever possible to avoid the hassle of traveling to and out of Los Angeles (LAX).. Coming to the Bay Area for college, I expected Oakland (OAK) and San Francisco (SFO) to share a similar dynamic. add another sentence here for a better flowHowever, I was surprised to hear that some of my friends preferred SFO over OAK. In this section, we will determine whether SFO is better than OAK and BUR is better than LAX.\nI follow a similar approach as before but select only flight records that contain our desired airports.\n\n\nCode\ndef compare_airports(airlines, airports, delays, flights):\n    \"\"\" Returns a DataFrame with delay rates by airport and airline. Indices follow the format 'airline airport.'\n    airlines' and airports must be lists. delays and flights are DataFrames as created above.\n    \"\"\"\n    proportions = pd.DataFrame()\n    for airline in airlines:\n        for origin in airports:\n            proportions[airline + \" \" + origin] = airline_delay_frequencies(airline,\n                            delays[delays['Origin'] == origin], flights[flights['Origin'] == origin])\n    return proportions\n\n# Construct a bar plot of flight delay percentage according to time\na = sns.lineplot(data=compare_airports(['Southwest Airlines Co.'], ['LAX', 'BUR', 'OAK', 'SFO'], delays_ma, flights_ma))\nplt.xlabel(\"Scheduled Hour of Flight Departure\")\nplt.ylabel(\"Percentage\")\nplt.title(\"Percent of flights delayed by airport, airline\")\nplt.xticks(range(0,24,4), ['12am','4am','8am','12pm','4pm','8pm'])\nsns.move_legend(a, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n\n\n\nWe see that BUR has fewer delays than LAX, OAK, and SFO. Burbank is better than LAX, but Oakland and SFO are about the same. Maybe my friends’ preference for SFO compared to OAK stems from something else, but clearly delays don’t play a major role in that decision."
  },
  {
    "objectID": "posts/Flights-technical.html#downloading-the-data",
    "href": "posts/Flights-technical.html#downloading-the-data",
    "title": "Everything to Know About Flight Delays",
    "section": "",
    "text": "The best resource to investigate the regularity of flights within the US is the Bureau of Transportation Statistics. They have a website that hosts the data, which is publicly available. The data is available by month, meaning that I had to manually request data for the 14 months I was interested in.\nWe can automate this download with the requests library of Python, setting START_YEAR to the beginning year of the period of interest and END_YEAR to the end year (exclusive).\n\n\nCode\nSTART_YEAR = 2022\nEND_YEAR = 2024\n\n\n\n\nCode\nos.mkdir(\"data\")\n\n\n\n\nCode\nfor i in range(START_YEAR, END_YEAR):\n    for j in range(1,13):\n        # We set verify to False because an SSL cert error gets thrown otherwise for some reason.\n        # For some reason, creating a pool manager as described on the Certifi documentation\n        # throws an SSL connection error. It could be an issue with Google Colab, which I am using to\n        # work with this notebook.\n        r = requests.get(f\"https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{i}_{j}.zip\", verify=False)\n        with open(f\"data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{i}_{j}.zip\", \"wb\") as fd:\n            fd.write(r.content)\n\n\nAt this point, we have a bunch of zip files, one for each month of flights. We’ll unzip them one-by-one.\n\n\nCode\n#Construct an array to hold our zip filepaths\nzips = []\nfor root, directories, filenames in os.walk('data'):\n    for directory in directories:\n         dpath = os.path.join(root, directory)\n\n    for filename in filenames:\n        fpath = os.path.join(root,filename)\n        if fpath[-3:] == 'zip':\n            zips.append(fpath)\n\nfor zip_path in zips:\n    try:\n        my_zip = zipfile.ZipFile(zip_path, 'r')\n        my_zip.extractall('data')\n    except zipfile.BadZipFile:\n        continue"
  },
  {
    "objectID": "posts/Flights-technical.html#reading-in-and-cleaning-data",
    "href": "posts/Flights-technical.html#reading-in-and-cleaning-data",
    "title": "Everything to Know About Flight Delays",
    "section": "",
    "text": "Now, we read the data in and clean it. We start out by reading in an arbitrary month to get a DataFrame with the correct columns, then loop through each month’s flight records (stored in a csv). We add each month’s flight records to our flights DataFrame; at this point, we are done loading data for this project.\nCleaning the DataFrames involves several steps. To start, we read in this HTML table from the Bureau of Transportation Statistics as a DataFrame. This table matches airlines with abbreviations. To aid legibility, we replace the abbreviations contained in our flights DataFrame with the airlines’ full names.\n\n\nCode\nimport re\n\n#Obtains DataFrame with correct columns\nflights = pd.DataFrame(columns=pd.read_csv(\"data/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_5.csv\").columns)\n\n#Concatenates inputs to flights DataFrame\nfor root, directories, filenames in os.walk('data'):\n    for filename in filenames:\n        fpath = os.path.join(root,filename)\n        if (fpath[-3:] == 'csv') & (fpath[5:8] == 'On_'):\n            a = pd.read_csv(fpath)\n\n            # Filter out unneeded rows (only keep things relevant to flight origin, airline, and departure delay)\n            a = a.filter(['CRSDepTime', 'DepDelayMinutes', 'Reporting_Airline', 'TaxiOut', 'Origin', 'Flight_Number_Reporting_Airline'], axis=1)\n\n            # Concatenate month table to flights\n            print(f\"  {fpath}\")\n            flights = pd.concat([flights, a], ignore_index=True, join='inner')\n\n# Obtains a DataFrame version of the HTML table on the BTS Airline Codes webpage\ncodes = pd.read_html(\"https://www.bts.gov/topics/airlines-and-airports/airline-codes\")[0]\nflights = flights.merge(right=codes, left_on='Reporting_Airline', right_on='Code', suffixes=['',''])#.drop([\"Reporting_Airline\", 'Code'], axis=1)\n\n\nWe now make a copy of flights called delays. We can now safely transform our data and keep the original flights available for later analysis. We’ll also regularize our time (converting the HHMM format to HH.MM, where MM is out of 100 instead of 60).\n\n\nCode\n# Makes a deep copy of flights called delays for transformation in this section. Filter out on-time flights.\nflights['CRSDepTime'] = flights['CRSDepTime'] % 100 * 5/3 * .01 + flights['CRSDepTime'] // 100\nflights['delay'] = flights['DepDelayMinutes'] &gt; 0\ndelays = flights.copy(deep=True)[flights['DepDelayMinutes'] &gt; 0]"
  },
  {
    "objectID": "posts/Flights-technical.html#plot-of-flight-delays-vs.-departure-time",
    "href": "posts/Flights-technical.html#plot-of-flight-delays-vs.-departure-time",
    "title": "Everything to Know About Flight Delays",
    "section": "",
    "text": "Let’s take a precursory view of flight delays in the aggregate and see how they correlate with departure time.\n\n\nCode\nsns.set_theme()\n\nf, ax = plt.subplots(figsize=(7, 7), sharex=True, sharey=True)\na = sns.histplot(data=delays, x='CRSDepTime', y='DepDelayMinutes', bins=[12,600],\n                 cmap=sns.color_palette(\"flare_r\", as_cmap=True), cbar=True, cbar_kws={'label': 'Quantity of delays'})\nsns.kdeplot(data=delays.sample(1000), x='CRSDepTime', y='DepDelayMinutes', levels=5, color=\"w\")\n\nplt.title('Density of delays by departure time, ' + str(START_YEAR) + \"-\" + str(END_YEAR - 1))\nplt.xlabel('Departure time')\nplt.ylabel('Departure delay, in minutes')\nplt.xlim((0, 24))\nplt.ylim(bottom=0, top=90)\nplt.xticks(range(0,24,4), ['12am','4am','8am','12pm','4pm','8pm'])\n\n\n([&lt;matplotlib.axis.XTick at 0x7fa796f36290&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa796f35c00&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7d57ca740&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7894ad390&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7894ac340&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7894ae500&gt;],\n [Text(0, 0, '12am'),\n  Text(4, 0, '4am'),\n  Text(8, 0, '8am'),\n  Text(12, 0, '12pm'),\n  Text(16, 0, '4pm'),\n  Text(20, 0, '8pm')])\n\n\n\n\n\nWe see that delays overall are densest between 2pm and 6pm, but the most common delay is less than 10 minutes and occurs between 10am and 12pm. Given that the most common delay is minimal, I wanted to take a better look. We’ll look at significant delays, which I will define to be greater than or equal to 30 minutes in length."
  },
  {
    "objectID": "posts/Flights-technical.html#plot-of-significant-and-nonsignificant-delays-vs.-departure",
    "href": "posts/Flights-technical.html#plot-of-significant-and-nonsignificant-delays-vs.-departure",
    "title": "Everything to Know About Flight Delays",
    "section": "",
    "text": "Code\ndef delay_significance(x):\n    \"\"\"Categorizes delays into one of three categories.\"\"\"\n    if x == 0:\n        return \"On Time\"\n    elif x &lt; 30 and x &gt; 0:\n        return \"Slight Delay\"\n    else:\n        return \"Significant Delay\"\n\n#Creates labels for hue sorting on histogram.\nflights['Delay Status'] = flights['DepDelayMinutes'].apply(delay_significance)\n\n\n\n\nCode\nsns.histplot(flights, x='CRSDepTime', kde=True, hue='Delay Status',\n             binwidth=1, kde_kws={'bw_adjust': 3}).set(xlabel='Departure time', title='Flights from '\n                             + str(START_YEAR) + ' to ' + str(END_YEAR - 1) + ', by hour')\n\nplt.xlim(0,24)\nplt.xticks(range(0,24,4), ['12am','4am','8am','12pm','4pm','8pm'])\n\n\n([&lt;matplotlib.axis.XTick at 0x7fa7849715d0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7849715a0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa784972a40&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7858f0b80&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7858f1630&gt;,\n  &lt;matplotlib.axis.XTick at 0x7fa7858f20e0&gt;],\n [Text(0, 0, '12am'),\n  Text(4, 0, '4am'),\n  Text(8, 0, '8am'),\n  Text(12, 0, '12pm'),\n  Text(16, 0, '4pm'),\n  Text(20, 0, '8pm')])\n\n\n\n\n\nFrom the chart, we see that both slight and significant delays increase as the day goes on. Significant delays start low at 6am and steadily increase, peaking at 6pm, while slight delays seem to plateau around 10am."
  },
  {
    "objectID": "posts/Flights-technical.html#delays-by-airline",
    "href": "posts/Flights-technical.html#delays-by-airline",
    "title": "Everything to Know About Flight Delays",
    "section": "",
    "text": "Now, let’s look at delays by airline. We merge wholly-owned subsidiaries with their parent company (a complete list of wholly-owned subsidiary airlines in North America can be found on Wikipedia), as people do not often see their brand nor purchase from them. Some airlines, such as Republic Airline, are regional airlines that fly under multiple airline names; our data does not represent whose banner they fly under for a given flight, making it impossible for us to merge their flights with their contracted carrier.\n\n\nCode\n# Adds new departure hour columns to flights and delays for easier charting\nflights['CRSDepHour'], delays['CRSDepHour'] = flights['CRSDepTime'] // 1,  delays['CRSDepTime'] // 1\n\n#Selects only for significant delays\ndelays = delays[delays['DepDelayMinutes'] &gt;= 30]\n\n# Merges wholly-owned subsidiary airlines with their parent companies\ndelays_ma = delays.replace(to_replace={\"Airline\": {\"Horizon Air\":\"Alaska Airlines Inc.\", \"Envoy Air\":\"American Airlines Inc.\", \"PSA Airlines Inc.\":\"American Airlines Inc.\", \"Endeavor Air Inc.\": \"Delta Air Lines Inc.\"}})\nflights_ma = flights.replace(to_replace={\"Airline\": {\"Horizon Air\":\"Alaska Airlines Inc.\", \"Envoy Air\":\"American Airlines Inc.\", \"PSA Airlines Inc.\":\"American Airlines Inc.\", \"Endeavor Air Inc.\": \"Delta Air Lines Inc.\"}})\n\ndef airline_delay_frequencies(airline, delays, flights):\n    \"\"\"Returns delay proportions, grouped by departure hour and departure time.\"\"\"\n    total_delays = delays[delays['Airline'] == airline].groupby('CRSDepHour')['CRSDepTime'].count()\n    total_flights = flights[flights['Airline'] == airline].groupby('CRSDepHour')['CRSDepTime'].count()\n    return (total_delays / total_flights).reindex(np.arange(24), fill_value=0) * 100\n\n# Accumulates multiple airline Series into a DataFrame for line plot\nairlines = ['American Airlines Inc.', #'SkyWest Airlines Inc.',\n       'Alaska Airlines Inc.', 'United Air Lines Inc.',\n       'Delta Air Lines Inc.', 'Frontier Airlines Inc.', #'Allegiant Air',\n       #'Hawaiian Airlines Inc.',\n        'Spirit Air Lines',\n       'Southwest Airlines Co.', #'Mesa Airlines Inc.', 'Republic Airline',\n       'JetBlue Airways']\nproportions = pd.DataFrame()\nfor airline in airlines:\n    proportions[airline] = airline_delay_frequencies(airline, delays_ma, flights_ma)\n\n\n\n\nCode\n# Constructs a bar plot of flight delay percentage according to time\na = sns.lineplot(data=proportions, palette=\"tab10\", linewidth=2.5)\nplt.xlabel(\"Scheduled Hour of Flight Departure\")\nplt.ylabel(\"Percentage\")\nplt.title(\"Percent of flights significantly delayed, by time of day\")\nplt.xticks(range(0,24,4), ['12am','4am','8am','12pm','4pm','8pm'])\nplt.ylim(0, 40)\nsns.move_legend(a, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n\n\n\nRegardless of airline, the chance of significant delay is lowest in the early hours of the morning, regardless of airline. From there, the probability of delay steadily increases and peaks in the evening. Then there is a dip around midnight, with delays skyrocketing in the wee hours of the morning. Many of these flights are budget airline red-eyes from Alaska and Puerto Rico that occur at the end of an airline’s work day, which can be later than 2am in those areas. Flying at the end of the work day increases the chance of delay since planes fly multiple flights in a day, and any delay in an earlier flight can mess up the traffic control schedule for all later flights. Additionally, if delayed for too long, crew duty hours can also exceed the limit. This happened in Southwest’s meltdow last year, where Southwest had insufficient replacement crews and misallocation of planes, leading to mass flight cancellations.\nThe budget airlines (Frontier, Spirit, Southwest, and JetBlue) all have higher rates of delay throughout the day according to this dataset, with the exception of Southwest, which has a delay rate comparable with the non-budget airlines in the morning. However, by 12pm, Southwest’s significant delay rate grows past that of the non-budget airlines and joins its budget peers by 4 or 5 pm.\nBy the numbers:\n\n\nCode\n(delays.groupby('Airline').count()[['CRSDepTime']] / flights.groupby('Airline').count()[['CRSDepTime']] * 100).sort_values('CRSDepTime', ascending=False).rename({'CRSDepTime':'% Flights Significantly Delayed'}, axis=1)\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\n% Flights Significantly Delayed\n\n\nAirline\n\n\n\n\n\nJetBlue Airways\n22.078514\n\n\nFrontier Airlines Inc.\n21.896298\n\n\nAllegiant Air\n18.991785\n\n\nSpirit Air Lines\n16.958591\n\n\nSouthwest Airlines Co.\n15.292280\n\n\nAmerican Airlines Inc.\n14.529106\n\n\nMesa Airlines Inc.\n13.838768\n\n\nUnited Air Lines Inc.\n12.994220\n\n\nHawaiian Airlines Inc.\n11.101448\n\n\nPSA Airlines Inc.\n11.007379\n\n\nDelta Air Lines Inc.\n10.671968\n\n\nSkyWest Airlines Inc.\n10.629637\n\n\nAlaska Airlines Inc.\n10.458504\n\n\nRepublic Airline\n10.361243\n\n\nEndeavor Air Inc.\n9.843149\n\n\nEnvoy Air\n9.762695\n\n\nHorizon Air\n8.873647"
  },
  {
    "objectID": "posts/Flights-technical.html#applications-lax-and-bur-and-sfo-and-oak",
    "href": "posts/Flights-technical.html#applications-lax-and-bur-and-sfo-and-oak",
    "title": "Everything to Know About Flight Delays",
    "section": "",
    "text": "Since starting college, I’ve traveled quite regularly on planes between my hometown in Southern California and my university in the San Francisco Bay Area. I was drawn to this topic to get a better sense of which airline and airport combination offered me the fewest overall delays. As a college student with a constrained budget, I often take Southwest to and from school due to its cheap fares and customer service..\nSouthern California common wisdom dictates that one should fly out of Burbank (BUR) whenever possible to avoid the hassle of traveling to and out of Los Angeles (LAX).. Coming to the Bay Area for college, I expected Oakland (OAK) and San Francisco (SFO) to share a similar dynamic. add another sentence here for a better flowHowever, I was surprised to hear that some of my friends preferred SFO over OAK. In this section, we will determine whether SFO is better than OAK and BUR is better than LAX.\nI follow a similar approach as before but select only flight records that contain our desired airports.\n\n\nCode\ndef compare_airports(airlines, airports, delays, flights):\n    \"\"\" Returns a DataFrame with delay rates by airport and airline. Indices follow the format 'airline airport.'\n    airlines' and airports must be lists. delays and flights are DataFrames as created above.\n    \"\"\"\n    proportions = pd.DataFrame()\n    for airline in airlines:\n        for origin in airports:\n            proportions[airline + \" \" + origin] = airline_delay_frequencies(airline,\n                            delays[delays['Origin'] == origin], flights[flights['Origin'] == origin])\n    return proportions\n\n# Construct a bar plot of flight delay percentage according to time\na = sns.lineplot(data=compare_airports(['Southwest Airlines Co.'], ['LAX', 'BUR', 'OAK', 'SFO'], delays_ma, flights_ma))\nplt.xlabel(\"Scheduled Hour of Flight Departure\")\nplt.ylabel(\"Percentage\")\nplt.title(\"Percent of flights delayed by airport, airline\")\nplt.xticks(range(0,24,4), ['12am','4am','8am','12pm','4pm','8pm'])\nsns.move_legend(a, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n\n\n\nWe see that BUR has fewer delays than LAX, OAK, and SFO. Burbank is better than LAX, but Oakland and SFO are about the same. Maybe my friends’ preference for SFO compared to OAK stems from something else, but clearly delays don’t play a major role in that decision."
  },
  {
    "objectID": "posts/avoiding-overfitting.html",
    "href": "posts/avoiding-overfitting.html",
    "title": "Avoiding overfitting in Lincoln-Douglas debate practice",
    "section": "",
    "text": "In this article, I hope to explain overfitting, a concept from the discipline of machine learning, and apply it to debate.\nOverfitting is traditionally defined as producing a model that is too particular and doesn’t accurately work with future problems.\nWe’ve all experienced overfitting before. Probably the most prominent debate example of this phenomenon is redoing speeches. For me, the process of redoing a speech went in the following order: first, I would produce redo material by doing a practice debate, looking through old flows, or watching a debate online. Then, I’d re-give the target speech several times, first with myself and then later with a coach to listen and give feedback. In the end, I’d always feel like I’d improved a lot. For example, I seriously drilled this 2NR from the 2019 National Debate Tournament and got really good at it.\nAfter that drill regiment, I expected I would be much better at giving topicality speeches, but I was disappointed to find that I was only good at producing that one speech, all the while not much better at giving other T 2NRs. In retrospect, the problem is clear: the sample size of topicality rounds at that point was miniscule (\\(n = 1\\)), so my model for giving topicality 2NRs was severely overfitted. Many of the arguments from that speech were contextual to the Executive Authority topic, so I was good at explaining the particularities of why it was hard to be neg in context of that resolution, but not in context of any others. After spelling it out like this, the solution seems to be obvious.\nI did what many of you are thinking: redo speeches in context of the current topic! I drilled several 2NRs on topicality, some of which I’ll link to at the end of this article. This allowed me to better abstract (or generalize) my knowledge about what good topicality debating was and thus allowed me to better apply that generalizable knowledge/intuition to future rounds. To make this advice more concrete, if I were looking to improve at topicality, I’d not only practice/block out the generic “limits outweighs aff counter-standards” but also “limits outweighs predictability,” “it’s hard to be neg for ____ reason,” and even standard-level answers like responses to the PICs objection to T-Bare Plurals, etc. To take it to the next level, I’d also drill different kinds of T arguments across different topics to improve the skill of contextualization (i.e. the AFF’s PICs objection might be more persuasive on a topic in which the States counterplan ran rampant, compared to one in which PICs weren’t good).\nIdeally, regardless of the style of debate being pursued, you would be best to eliminate all “unknown unknowns” (https://en.wikipedia.org/wiki/There_are_known_knowns), or things that catch you by complete surprise. Once you do this, you’ll start finding patterns. For example, the PICs objection applies to most topicality arguments that argue that a type of specification is bad (some that come to mind are T-Plural, T-Eliminate, T-Substantial, T-Nearly All, T-The, T-In, etc.), so with a few modifications, frontlines should be cross-applicable between some T debates.\nA word of caution to taking pattern focus too far: you will inevitably lose specificity and contextualization. This is what judges complain about when they say that topicality debates are becoming stale. It’s one thing to know the generic 7-point response to the aff’s PICs objection, but knowing how to make the all-purpose block more specific (and thus far more persuasive) is an entirely different beast. For instance, quantitative limits claims for Nebel T on the 2019 January-February topic about authoritarian regimes may be less applicable to this year’s January-February topic, which deals with far fewer countries (compare 50 regimes on the JF19 topic with 9 countries on the JF20 topic). This should tell that even the same topicality violation should be debated very differently depending on topic. The difference between Nebel T and an entirely different violation, say T-Plural, is even starker!\nIn sum, you should disrupt your brain to prevent overfitting. If you’re getting better at beating case dumps that you’ve put together on your own, ask a coach or a teammate to put together new ones with random cards from around the wiki. Take the case dumps and do them in sequence, focusing on the quality of the first attempt. Obviously, the unfamiliar ones will be harder than the familiar ones, but ideally, you should be working to close the gap between them. This can help you practice getting used to the unfamiliarity of any good 1NC case strategy, because in a real round, your speech should be great the first time, not the tenth. Sample drill ideas:\n\nDrill the topicality rounds below.\nGo through each Lincoln-Douglas topic that you have debated (or college policy topics, if you know how a topic’s meta breaks down) and write down specificities that complicate generic responses (i.e. the dominance of a core negative generic, lack of disadvantage ground, resolution vagueness, hypothetical quantitative number of affirmatives). Then, try to give speeches while noting these nuances. Rounds that I drilled:\nMontgomery Bell Academy GH vs Monta Vista PS at the 2018 Cal RR. Note: the blocks read here are stale because since 2018 people have been transcribing and reading them.\nNorthwestern JW vs Kentucky BT at the 2019 NDT\nMany of Ishan Bhatt’s (unlisted ☹). A few more topicality rounds that may be helpful:\nHarvard-Westlake SM vs Archbishop Mitty JP at the 2020 Stanford Invitational"
  },
  {
    "objectID": "posts/amazon-images.html",
    "href": "posts/amazon-images.html",
    "title": "Amazon Photos collects your photo data – you can disable this",
    "section": "",
    "text": "Amazon Photo grants unlimited photo storage (and 5GB of video storage) to Prime members, which is why I use it to store my photo backups.\nI was peeking around in the Amazon Photos Terms and Conditions recently when I found this:\n\nWe retain image recognition data relating to the photos and videos you store using the Services until you disable the applicable Tagging Feature or your account’s access to the Services is terminated.\n\nAlthough vague, this language suggests that your photos and videos could become training data for their models. To avoid your photos’ and videos’ inclusion, go to Amazon Photo Settings, click on Image Recognition, and toggle Tag Photos to OFF."
  },
  {
    "objectID": "posts/taiwanese_music_1.html",
    "href": "posts/taiwanese_music_1.html",
    "title": "Notes on Taiwanese Indie Music",
    "section": "",
    "text": "Since leaving Taiwan, I’ve delved into the island’s indie music scene, taking recommendations mainly from PTT (Taiwan’s Reddit equivalent), my friend Yang Lee, and the Spotify algorithm. The selections I’ve listed below are eclectic, ranging from siren-like female vocals over jazz piano to poppy, guitar-heavy satire of Japanese magical girl anime. Taiwan’s alternative scene is not merely derivative of Japan’s, the West’s, or anywhere else’s—the music is creative and unique, reminding me of the enormity and potential of what’s still out there to discover.\nNote: Translations are mine. Search for songs using the Chinese name if available. I made a Spotify playlist with every song in this reflection, plus many more.\n\nHello Nico\n\n\n\nImage source: Taiwan Beats\n\n\nHello Nico is not a loud band, but their songs are no less powerful because of it. The lyrics take the form of recollections, looking doubtfully at past decisions. Their lead singer’s soft voice makes for good “sad” music, but they have lots of optimistic music that is at least as good. Selections:\n\nFlower/Hua 花 is a slow, apologetic contemplation of what could have been, had the speaker not unintentionally hurt Hua (a pet name for the subject). Spotify, YouTube\nUnfamiliar Room, Unfamiliar Afternoon 陌生的房間陌生的下午 describes the pain of naively continuing an unreciprocated love. Spotify, YouTube\nFacing Yourself 面向自己 exemplifies “breathiness” in Taiwanese music (which is popular in the Japanese scene) and intelligently uses myriad color and light-based metaphors to describe the uniqueness of the speaker’s romantic interest. Spotify, YouTube\n\n\n\nIruka Porisu (aka Dolphin Police, イルカポリス, 海豚刑警)\n\n\n\nImage source: Zhihu\n\n\nIruka Porisu’s 2019 album Call Me When Night Go Blue is one of my favorites of 2023. Songs effortlessly mesh the lead singer Wuyue’s (伍悅) siren-like voice, intense guitar solos, and unconventional instruments such as a kid’s piano. Their music, lyrics, social media profiles, and videos embody youth—irreverent, chaotic, and joyful—and remind me to enjoy mine while it lasts. Check out their music videos, which I’ve linked below.\nSelections:\n\nLights of Anping 安平之光 describes blithely riding a Vespa around Anping, Tainan (a city in the south of Taiwan), away from depressing Taipei. Spotify, Youtube\nTell Me I Was in Your Dream 當ㄋ沉睡ㄉ時候告訴婐ㄋ夢到ㄌ婐 is an enthralling song about love and longing in a relationship with a pessimist. Spotify, YouTube, YouTube acoustic\nYoung Folks Die Late (original song title is in English) is full of harmonic tension and discusses the previous night’s sexual partner for misunderstanding the speaker. Spotify, YouTube\nBadminton Youth 羽球少年 describes the speaker’s romantic frustrations with men, women, and the available partners in her environment, especially those available on Tinder. Spotify, YouTube\n\n\n\nElephant Gym (大象體操)\n\n\n\nImage source: Taiwan Beats\n\n\nWithin my first month in Taiwan, shortly after discovering Elephant Gym on PTT, I saw them perform at an annual memorial concert that raises awareness for the 228 Incident, an anti-government uprising in 1948 that was violently suppressed by the Chiang Kai-shek regime. Recently, they’ve skyrocketed in popularity in the West and have even been selected as the cover image for Spotify’s global math rock playlist. The trio expertly combines melodies with polyrhythms on guitar, base, and drums, and sometimes even adds in female vocals. Words rarely appear on tracks, and when they do, they are sparing. Hence, I have omitted descriptions of lyrics for this artist.\nThey’re currently on their World Tour. If anyone is interested in seeing the Los Angeles or San Francisco shows, I have tickets to both.\nSelections:\n\nMidway 中途 Spotify, YouTube\nGo Through the Night 穿過夜晚 Spotify, YouTube\nUnderwater 水底 Spotify, YouTube\n\n\n\nSweet John (甜約翰)\n\n\n\nImage source: Musictalk\n\n\nSweet John makes love songs that are introverted in character and content. The music is largely absent of the hysteria and bold proclamations typical of the genre, an interviewer noted in early 2020. Their lyrics instead provide a more realistic treatment of feelings like the anxiety inherent to an ambiguous relationship and of falling out of love in a relationship.\nSelections:\n\nThe Chance of Rainfall 降雨機率 has gotten universal acclaim from everyone I’ve shown it to thus far. “Why can’t Americans make more songs like this?” was my dad’s reaction. Its jazz-inspired instrumentals are as strong as its duet vocals and makes a great introduction to this genre. Spotify, YouTube\nSafety Limit 安全範圍 weaves dramatic guitar riffs and drum rhythms together with a violin part and the singers’ more delicate voices. Spotify, YouTube\nSpark Fades 容易被厭倦的時刻 ponders the longevity of the speaker’s relationship with his significant other. It begins simply with vocals and piano but gradually evolves into an intense guitar and drum-driven climax. Spotify, YouTube"
  },
  {
    "objectID": "posts/embedded-clash.html",
    "href": "posts/embedded-clash.html",
    "title": "Embedded Clash - a Primer",
    "section": "",
    "text": "Embedded clash is a specific method of shortening of transitions on the line-by-line. I’ll give an example that illustrates what I mean.\nSay you have the following flow on a sample 1NC Case page with the following carded arguments (I’ll only show the tags for brevity):\n\nNo China war Circumvention – Trump hates the plan and won’t comply Turn – terrorists steal nuclear weapons in the process of elimination and detonate them\n\nThis case hit, although flawed in its brevity, is emblematic of what many debaters read on JF20. Now presume that you are to answer this case hit. For many, I’m sure that it would sound something like\n\n“They say no China war but they’re wrong – China war is real because…” “They say Trump hates the plan but…” “They say terrorists steal nukes but…”\n\nWhile this phrasing can be useful in some situations, especially when your opponent is unclear/when you can tell your judge is having trouble following along, it’s preferable in most cases to use the following phrasing:\n\n“Yes China war – …“\n\n\n“No circumvention – …”\n\n\n“Terrorists don’t steal nukes – …”\n\nI know it looks like a small change, but I can’t understate its importance. This tool not only will improve your speeches on a technical level, but it will also make you sound more argumentatively formidable. Past a certain skill level embedded clash becomes the default, and not employing it will reduce your speaker points and presence.\nFrom the given example, some may assume it’s only relevant to doing very technical case work, but it’s actually very applicable to any line-by-line analysis. Whether giving the 2NR on topicality, the critique, a disadvantage, or a counterplan, or even a lengthy case dump, it would be best in all cases to practice this style and best in most rounds to employ this method of delivery.\nNote: it may be better to use the former phrasing in a few situations: first, when a judge looks confused or is visibly having trouble flowing you; second, against an opponent who doesn’t signpost well/jumps around a lot; third, when you have to jump around a lot or are making more “overview-y” claims rather than mostly doing a lot of line-by-line work."
  },
  {
    "objectID": "posts/flights-nontechnical.html",
    "href": "posts/flights-nontechnical.html",
    "title": "Everything to Know About Flight Delays (no code)",
    "section": "",
    "text": "As a teenager, when booking flights, my mom always insisted that we book the earliest flight possible. It made no sense to me. “But we’ll have wasted a day traveling anyway, Mom! Why should we wake up super early and be tired just to get to the hotel and do nothing? Why not just book a flight for later in the day?” Years later, I saw her have the same argument with my sister. Having studied data science at UC Berkeley, I realized that this question was answerable with more than just anecdotes about delays.\nThat’s why I decided to study flight delays in-depth and write an article summarizing my conclusions. This article analyzes post-Covid flight information up to February 2023 (the latest available as of today) while sparing the reader the details of data processing."
  },
  {
    "objectID": "posts/flights-nontechnical.html#downloading-the-data",
    "href": "posts/flights-nontechnical.html#downloading-the-data",
    "title": "Everything to Know About Flight Delays (no code)",
    "section": "Downloading the data",
    "text": "Downloading the data\nThe best resource to investigate the regularity of flights within the US is the Bureau of Transportation Statistics. They have a website that hosts the data, which is publicly available. The data is available by month, meaning that I had to manually request data for the 14 months I was interested in.\nI automated data downloading using Python (a common programming language) to request the data in batches directly from the government server. Then, I compiled the data into a spreadsheet with the data processing tool pandas and did some additional processing to make later operations easier."
  },
  {
    "objectID": "posts/flights-nontechnical.html#plot-of-flight-delays-vs-departure-time",
    "href": "posts/flights-nontechnical.html#plot-of-flight-delays-vs-departure-time",
    "title": "Everything to Know About Flight Delays (no code)",
    "section": "Plot of flight delays vs departure time",
    "text": "Plot of flight delays vs departure time\nLet’s take a precursory view of flight delays in the aggregate and see how they correlate with departure time.\n\n\n\nFigure 1\n\n\nWe see that delays overall are densest between 2pm and 6pm, but the most common delay is less than 10 minutes and occurs between 10am and 12pm. Given that the most common delay is minimal, I wanted to take a better look. We’ll look at significant delays, which I will define to be greater than or equal to 30 minutes in length."
  },
  {
    "objectID": "posts/flights-nontechnical.html#plot-of-significant-and-nonsignificant-delays-vs-departure",
    "href": "posts/flights-nontechnical.html#plot-of-significant-and-nonsignificant-delays-vs-departure",
    "title": "Everything to Know About Flight Delays (no code)",
    "section": "Plot of significant and nonsignificant delays vs departure",
    "text": "Plot of significant and nonsignificant delays vs departure\n\n\n\nFigure 2\n\n\nFrom the chart, we see that both slight and significant delays increase as the day goes on. Significant delays start low at 6am and steadily increase, peaking at 6pm, while slight delays seem to plateau around 10am."
  },
  {
    "objectID": "posts/flights-nontechnical.html#delays-by-airline",
    "href": "posts/flights-nontechnical.html#delays-by-airline",
    "title": "Everything to Know About Flight Delays (no code)",
    "section": "Delays by airline",
    "text": "Delays by airline\nNow, let’s look at delays by airline. I merged wholly-owned subsidiaries with their parent company (a complete list of wholly-owned subsidiary airlines in North America can be found on Wikipedia), as people do not often see their brand nor purchase from them. Some airlines, such as Republic Airline, are regional airlines that fly under multiple airline names; our data does not represent whose banner they fly under for a given flight, making it impossible for us to merge their flights with their contracted carrier.\n\n\n\nFigure 3\n\n\nRegardless of airline, the chance of significant delay is lowest in the early hours of the morning, regardless of airline. From there, the probability of delay steadily increases and peaks in the evening. Then there is a dip around midnight, with delays skyrocketing in the wee hours of the morning. Many of these flights are red-eyes from Alaska and Puerto Rico that occur at the end of an airline’s work day. Flying at the end of the work day increases the chance of delay since planes fly multiple flights in a day, and any delay in an earlier flight can mess up the traffic control schedule for all later flights. Additionally, if delayed for too long, crew duty hours can also exceed the limit. This happened in Southwest’s meltdown last year, where Southwest had insufficient replacement crews and misallocation of planes, leading to mass flight cancellations.\nThe budget airlines (Frontier, Spirit, Southwest, and JetBlue) all have higher rates of delay throughout the day according to this dataset, with the exception of Southwest, which has a delay rate comparable with the non-budget airlines in the morning. However, by 12pm, Southwest’s significant delay rate grows past that of the non-budget airlines and joins its budget peers by 4 or 5 pm.\nBy the numbers:\n\n\n\n\n\nAirline\n\n\n% Flights Significantly Delayed\n\n\n\n\n\n\nJetBlue Airways\n\n\n22.078514\n\n\n\n\nFrontier Airlines Inc.\n\n\n21.896298\n\n\n\n\nAllegiant Air\n\n\n18.991785\n\n\n\n\nSpirit Air Lines\n\n\n16.958591\n\n\n\n\nSouthwest Airlines Co.\n\n\n15.292280\n\n\n\n\nAmerican Airlines Inc.\n\n\n14.529106\n\n\n\n\nMesa Airlines Inc.\n\n\n13.838768\n\n\n\n\nUnited Air Lines Inc.\n\n\n12.994220\n\n\n\n\nHawaiian Airlines Inc.\n\n\n11.101448\n\n\n\n\nPSA Airlines Inc.\n\n\n11.007379\n\n\n\n\nDelta Air Lines Inc.\n\n\n10.671968\n\n\n\n\nSkyWest Airlines Inc.\n\n\n10.629637\n\n\n\n\nAlaska Airlines Inc.\n\n\n10.458504\n\n\n\n\nRepublic Airline\n\n\n10.361243\n\n\n\n\nEndeavor Air Inc.\n\n\n9.843149\n\n\n\n\nEnvoy Air\n\n\n9.762695\n\n\n\n\nHorizon Air\n\n\n8.873647\n\n\n\n\n\nIn sum, to minimize the chance of delay, it’s best to choose a non-budget airline and depart as early as possible to avoid a significant delay. Your chances of making a connecting flight, getting to your destination at a reasonable hour, and enjoying a warm, healthy dinner all increase if you leave earlier in the day. It seems like my mom was right after all."
  },
  {
    "objectID": "posts/flights-nontechnical.html#applications-lax-and-bur-and-sfo-and-oak",
    "href": "posts/flights-nontechnical.html#applications-lax-and-bur-and-sfo-and-oak",
    "title": "Everything to Know About Flight Delays (no code)",
    "section": "Applications: LAX and BUR, and SFO and OAK",
    "text": "Applications: LAX and BUR, and SFO and OAK\nSince starting college, I’ve traveled quite regularly on planes between my hometown in Southern California and my university in the San Francisco Bay Area. I was drawn to this topic to get a better sense of which airline and airport combination offered me the fewest overall delays. As a college student with a constrained budget, I often take Southwest to and from school due to its cheap fares and customer service..\nSouthern California common wisdom dictates that one should fly out of Burbank (BUR) whenever possible to avoid the hassle of traveling to and out of Los Angeles (LAX.. Coming to the Bay Area for college, I expected Oakland (OAK) and San Francisco (SFO) to share a similar dynamic. However, I was surprised to hear that some of my friends preferred SFO over OAK. In this section, we will determine whether SFO is better than OAK and BUR is better than LAX.\nI follow a similar approach as before but select only flight records that contain our desired airports.\n\n\n\nFigure 4\n\n\nWe see that BUR has fewer delays than LAX, OAK, and SFO. Burbank is better than LAX, but Oakland and SFO are about the same. Maybe my friends’ preference for SFO compared to OAK stems from something else, but clearly delays don’t play a major role in that decision."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ethan Elasky",
    "section": "",
    "text": "I am cofounder of Acaceta, which is a knowledge solutions platform for medical sales representatives and doctors.\nI am currently Research Assistant at Academia Sinica in the Lab of Professor Lun-Wei Ku, where I am researching retrieval-augmented generation strategies and debate between language models as a means of finding truth.\nMy undergraduate degree (major: Data Science, concentration: Applied Mathematics and Modeling, minor: Chinese Language) is from the University of California, Berkeley. At Berkeley, I wrote an honors thesis, in which I applied natural language processing (NLP) techniques to quantify media bias in Taiwan. I was advised by Lucy Li.\nAt the Haas School of Business, I was a member of Angus Hildreth’s Social Psychology and Business Lab for four semesters (Spring 2022-Spring 2024).\nI understand English and Mandarin. I studied abroad in Taiwan in Spring 2023 and have returned to Taiwan full-time for work.\nIn high school, I was ranked top-50 in Lincoln-Douglas debate nationally, and coached for DebateDrills, a nationally successful team, during my freshman and sophomore years of college."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Ethan Elasky",
    "section": "",
    "text": "I am cofounder of Acaceta, which is a knowledge solutions platform for medical sales representatives and doctors.\nI am currently Research Assistant at Academia Sinica in the Lab of Professor Lun-Wei Ku, where I am researching retrieval-augmented generation strategies and debate between language models as a means of finding truth.\nMy undergraduate degree (major: Data Science, concentration: Applied Mathematics and Modeling, minor: Chinese Language) is from the University of California, Berkeley. At Berkeley, I wrote an honors thesis, in which I applied natural language processing (NLP) techniques to quantify media bias in Taiwan. I was advised by Lucy Li.\nAt the Haas School of Business, I was a member of Angus Hildreth’s Social Psychology and Business Lab for four semesters (Spring 2022-Spring 2024).\nI understand English and Mandarin. I studied abroad in Taiwan in Spring 2023 and have returned to Taiwan full-time for work.\nIn high school, I was ranked top-50 in Lincoln-Douglas debate nationally, and coached for DebateDrills, a nationally successful team, during my freshman and sophomore years of college."
  }
]